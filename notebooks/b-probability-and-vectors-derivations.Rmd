---
title: "Appendix B: Probability and Vectors"
output:
  html_document:
    self_contained: true
---

#### Bernoulli distribution example

A random variable $Y$ is said to have a Bernoulli distribution with probability $\pi$, denoted $Y\sim \mathsf{Bernoulli}(\pi)$, if:

-   $\mathcal{S} = \{0, 1\}$
-   $P(Y = 1) = \pi$, where $\pi\in (0,1)$.

**Bernoulli PMF:** $$f_Y(y) = \pi^y (1-\pi)^{(1-y)}.$$

**Mean:** $$E(Y)=0(1-\pi )+1(\pi)\\=\pi.$$

**Variance** $$\mathrm{var}(Y)=(0-\pi)^2(1-\pi)+(1-\pi)^2\pi \\= \pi(1-\pi).$$

#### Example (Exponential distribution)

A random variable $Y$ is said to have an exponential distribution rate parameter $\lambda$, denoted with $Y \sim \mathsf{Exp}(\lambda)$ if $\mathcal{S} = {y\in \mathbb{R}:y\geq 0}$ and has distribution,

**Exponential PDF:**

$$f_Y(y)=\lambda\exp(-\lambda y)$$

**Mean:** $$
\begin{aligned}
E(Y)&=\int_{0}^{\infty} y\lambda \exp(-\lambda y)\;dy\\
&=-\exp(-\lambda y)(\lambda^{-1}+y)\biggr]^{\infty}_{0}\\
&=\frac{1}{\lambda}
\end{aligned}
$$

Note that this process involves integration by parts, which is not shown. Similarly, $E(Y^2)=\frac{2}{\lambda^{2}}$. Thus,

**Variance:** $$
\begin{aligned}
\mathrm{var}(Y)&=
E(Y^2)-[E(Y)]^2\\
&=2\lambda^{-2}-[\lambda^{-1}]^2\\
&=\frac{2}{\lambda^{2}}
\end{aligned}
$$

### Binomial distribution example

A random variable $Y$ is said to have a Binomial distribution with $n$ trials and probability of success $\theta$, denoted $Y\sim \mathsf{Bin}(n,\theta)$ when $\mathcal{S}=\{0,1,2,\ldots,n\}$ and the pmf is:

**Binomial PMF:**

$$f(y\mid\theta) = \binom{n}{y} \theta^y (1-\theta)^{(n-y)}.$$

An alternative explanation of a Binomial random variable is that it is the sum of $n$ independent and identically-distributed Bernoulli random variables. Alternatively, let $Y_1,Y_2,\ldots,Y_n\stackrel{i.i.d.}{\sim} \mathsf{Bernoulli}(\theta)$, where i.i.d. stands for independent and identically distributed, i.e., $Y_1, Y_2, \ldots, Y_n$ are independent random variables with identical distributions. Then $Y=\sum_{i=1}^n Y_i \sim \mathsf{Bin}(n,\theta)$.

A Binomial random variable with $\theta = 0.5$ models the question: what is the probability of flipping $y$ heads in $n$ flips?

Using this information and the facts above, we can easily determine the mean and variance of $Y$.

**Mean:**

$E(Y_i) = \theta$ for $i=1,2,\ldots,n$.

**Variance:**

$\mathrm{var}(Y_i)=\theta(1-\theta)$ for $i=1,2,\ldots,n$.

We determine that: $$
\begin{aligned}
E(Y)&=E\biggl(\sum_{i=1}^n Y_i\biggr)\\
&=\sum_{i=1}^n E(Y_i) \\
&= \sum_{i=1}^n \theta \\
&= n\theta.
\end{aligned}
$$

Similarly, since $Y_1, Y_2, \ldots, Y_n$ are i.i.d., we see that $$
\begin{aligned}
\mathrm{var}(Y) &= \mathrm{var}(\sum_{i=1}^n Y_i) \\
&= \sum_{i=1}^n\mathrm{var}(Y_i)\\
&=\sum_{i=1}^n \theta(1-\theta) \\
&= n\theta(1-\theta)
\end{aligned}
$$

### Continuous bivariate distribution example

Hydration is important for health. Like many people, the author has a water bottle he uses to say hydrated through the day and drinks several liters of water per day. Let's say the author refills his water bottle every 3 hours.

-   Let $Y$ denote the proportion of the water bottle filled with water at the beginning of the 3-hour window.
-   Let $X$ denote the amount of water the author consumes in the 3-hour window (measured in the the proportion of total water bottle capacity.

We know that $0\leq X \leq Y \leq 1$. The joint density of the random variables is

$$
f(x,y)=4y^2,\quad 0 \leq x\leq y\leq 1,
$$ and 0 otherwise.

We answer a series of questions about this distribution.

**Q1: Determine** $P(0.5\leq X\leq 1, 0.75\leq Y)$.

Note that the comma between the two events means "and".

Since $X$ must be no more than $Y$, we can answer this question as

$$
\int_{0.75}^{1} \int_{0.5}^{y} 4y^2\;dx\;dy=229/768\approx 0.30.
$$

**Q2: Determine the marginal distributions of** $X$ and $Y$.

To find the marginal distribution of $X$, we must integrate the joint pdf with respect to the limits of $Y$. Don't forget to include the support of the pdf of $X$ (which after integrating out $Y$, must be between 0 and 1). $$
\begin{aligned}
f_X(x) &=\int_{x}^1 4y^2\;dy \\
&=\frac{4}{3}(1-x^3),\quad 0\leq x \leq 1.
\end{aligned}
$$

Similarly, $$
\begin{aligned}
f_Y(y) &=\int_{0}^y 4y^2\;dx \\
&=4y^3,\quad 0\leq y \leq 1.
\end{aligned}
$$

**Q3: Determine the means of** $X$ and $Y$.

The mean of $X$ is the integral of $x f_X(x)$ over the support of $X$, i.e., $$
E(X) =\int_{0}^1 x\biggl(\frac{4}{3}(1-x^3)\biggr)\;dx = \frac{2}{5}
$$

Similarly, $$
E(Y) =\int_{0}^1 y(4y^3)\;dy = \frac{4}{5}
$$

**Q4: Determine the variances of** $X$ and $Y$.

We use the formula $\mathrm{var}(X)=E(X^2)-[E(X)^2]$ to compute the variances. First, $$
E(X^2) =\int_{0}^1 x^2\biggl(\frac{4}{3}(1-x^3)\biggr)\;dx = \frac{2}{9}
$$ Second, $$
E(Y^2) =\int_{0}^1 y^2(4y^3)\;dy = \frac{2}{3}
$$ Thus,

$$\mathrm{var}(X)=2/5-(2/9)^2=\frac{142}{405}$$ $$\mathrm{var}(Y)=4/5-(2/3)^2=\frac{16}{45}$$

**Q5: Determine the mean of** $XY$.

The mean of $XY$ requires us to integrate the product of $xy$ and the joint pdf over the joint support of $X$ and $Y$. Specifically, $$
E(XY)=\int_{0}^{1}\int_{0}^{y} xy(4y^2)\;dx\;dy= \frac{1}{12}
$$

**Q6: Determine the covariance of** $X$ and $Y$.

Using our previous work, we see that, $$
\begin{aligned}
\mathrm{cov}(X,Y)&=E(XY) - E(X)E(Y)\\&=1/12-(2/5)(4/5)\\&=-\frac{71}{300}
\end{aligned}
$$

**Q7: Determine the mean and variance of** $Y-X$, i.e., the average amount of water remaining after a 3-hour window and the variability of that amount.

Using the results about the expected value of a sum of random variables, we have that, $$
\begin{aligned}
E(Y-X)&=E(Y)-E(X)\\&=4/5-2/5\\&=\frac{2}{5}
\end{aligned}
$$ and, $$
\begin{aligned}
\mathrm{var}(Y-X)&=\mathrm{var}(Y)+\mathrm{var}(X)-2\mathrm{cov}(Y,X)\\&=
16/45+142/405-2(-71/300)\\&\approx 1.18.
\end{aligned}
$$

### Continuous bivariate distribution example continued

Using the definitions we introduced, we want to answer **Q7** of the hydration example. Summarizing only the essential details, we have a random vector $\mathbf{z}=[X, Y]$ with mean $E(\mathbf{z})=[2/5, 4/5]$ and covariance matrix $$
\mathrm{var}(\mathbf{z})=
\begin{bmatrix}
142/405 & -71/300 \\
-71/300 & 16/45
\end{bmatrix}.
$$ We want to determine $E(Y-X)$ and $\mathrm{var}(Y-X)$.

Define $\mathbf{A}=[-1, 1]^T$ (the ROW vector with 1 and -1). Then, $$
\mathbf{Az}=\begin{bmatrix}-1 & 1\end{bmatrix}
\begin{bmatrix}
X\\
Y
\end{bmatrix}
=Y-X
$$ and, $$
\begin{aligned}
E(Y-X)&=E(\mathbf{Az})\\
&=\begin{bmatrix}-1 & 1\end{bmatrix}
\begin{bmatrix}
2/5\\
4/5
\end{bmatrix}\\
&=-2/5+4/5\\&=2/5.
\end{aligned}
$$ Additionally, $$
\begin{aligned}
& \mathrm{var}(Y-X) \\
&=\mathrm{var}(\mathbf{Az}) \\
&=\mathbf{A}\mathbf{var}(\mathbf{z})\mathbf{A}^T \\
&=
\begin{bmatrix}
-1 & 1
\end{bmatrix}
\begin{bmatrix}
142/405 & -71/300 \\
-71/300 & 16/45
\end{bmatrix}
\begin{bmatrix}
-1 \\ 1
\end{bmatrix} \\
&= \begin{bmatrix}
-142/405-71/300 & 71/300+16/45
\end{bmatrix}
\begin{bmatrix}
-1 \\ 1
\end{bmatrix} \\
&= 142/405 + 16/45 + 2(71/300) \\
&\approx 1.18.
\end{aligned}
$$

### OLS matrix-form example

Ordinary least squares regression is a method for fitting a linear regression model to data. Suppose that we have observed variables $X_1, X_2, X_3, \ldots, X_{p-1}, Y$ for each of $n$ subjects from some population, with $X_{i,j}$ denoting the value of $X_j$ for observation $i$ and $Y_i$ denoting the value of $Y$ for observation $i$. In general, we want to use $X_1, \ldots, X_{p-1}$ to predict the value of $Y$. Let, $$
\mathbf{X} =
\begin{bmatrix}
1 & X_{1,1} & X_{1,2} & \cdots & X_{1,n} \\
1 & X_{2,1} & X_{2,2} & \cdots & X_{2,n} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & X_{n,1} & X_{n,2} & \cdots & X_{n,n}
\end{bmatrix}
$$ be a full-rank matrix of size $n\times p$ and $$
\mathbf{y}=(Y_1, Y_2, \ldots,Y_n)^T,
$$ be an $n\times 1$ vector of responses. It is common to assume that, $$ \mathbf{y}\sim \mathsf{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_{n\times n}),
$$
where $\beta=(\beta_0,\beta_1,\ldots,\beta_{p-1})$ is a $p$-dimensional vector of constants.

The matrix $\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$ projects $\mathbf{y}$ into the space spanned by the vectors in $\mathbf{X}$. Because of what we know about linear functions of a multivariate normal random vector, we can determine that, $$
\mathbf{Hy}\sim \mathsf{N}(\mathbf{X}\boldsymbol{\beta},\sigma^2 \mathbf{H}).
$$
