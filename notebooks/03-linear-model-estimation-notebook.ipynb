{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3 - Linear Model Estimation\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click the Open in Colab graphic below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/03-linear-model-estimation-notebook.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"> </a>\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "c0106f3b-1997-476a-88e4-dddd5426904e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(!require(palmerpenguins, quietly = TRUE)) {\n",
        "  install.packages(\"palmerpenguins\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(palmerpenguins)\n",
        "}\n",
        "if(!require(ggplot2, quietly = TRUE)) {\n",
        "  install.packages(\"ggplot2\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(ggplot2)\n",
        "}"
      ],
      "id": "5070824f-3749-4d1d-9b58-8fedc48cc453"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A simple motivating example\n",
        "\n",
        "Suppose you observe data related to the heights (in) of 5 mothers and their adult daughters. The data are in the table below.\n",
        "\n",
        "| observation | mother | daughter |\n",
        "|------------:|-------:|---------:|\n",
        "|           1 |   57.5 |     61.5 |\n",
        "|           2 |   60.5 |     63.5 |\n",
        "|           3 |   63.5 |     63.5 |\n",
        "|           4 |   66.5 |     66.5 |\n",
        "|           5 |   69.5 |     66.5 |\n",
        "\n",
        "Would it be reasonable to use a mother’s height to predict the height of her adult daughter? Consider the plot below."
      ],
      "id": "be8e8a0f-2766-4351-9c71-43c89d9f0412"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "x <- c(57.5, 50.5, 63.5, 66.5, 69.5) # mothers' heights\n",
        "y <- c(61.5, 63.5, 63.5, 66.5, 66.5) # daughters' heights\n",
        "plot(y ~ x, pch = 19, xlab = \"mother's height (in)\", ylab = \"daughter's height (in)\")"
      ],
      "id": "134cb3b3-876b-4103-8c8e-bde80b57453f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What is regression?**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "A **regression analysis** is the process of building a model describing the typical relationship between a set of observed variables.\n",
        "\n",
        "-   A regression analysis builds the model using observed values of the variables for $n$ subjects sampled from a population.\n",
        "-   In our example, we want to build a **regression** model for the height of adult daughters using their height of their mothers.\n",
        "\n",
        "**Response versus predictor variables**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The variables in a regression analysis may be divided into two types:\n",
        "\n",
        "-   The response variable.\n",
        "-   The predictor variables.\n",
        "\n",
        "The outcome variable we are trying to predict is known as the **response variable**.\n",
        "\n",
        "-   Response variables are also known as **outcome**, **output**, or **dependent** variables.\n",
        "-   The response variable is denoted by $Y$.\n",
        "-   $Y_i$ denotes the value of $Y$ for observation $i$.\n",
        "\n",
        "The variables available to model the response variable are known as **predictors variables**:\n",
        "\n",
        "-   They are also called **explanatory**, **regressor**, **input**, **independent** variables or simply as **features**.\n",
        "-   Following the convention of Weisberg (2014), we use the term **regressor** to refer to the variables used in our regression model, whether that is the original predictor variable, some transformation of a predictor, some combination of predictors, etc.\n",
        "-   Every predictor can be a regressor but not all regressors are a predictor.\n",
        "-   The regressor variables are denoted as $X_1, X_2, \\ldots, X_{p-1}$.\n",
        "-   $x_{i,j}$ denotes the value of $X_j$ for observation $i$\n",
        "-   If there is only a single regressor in the model, we can denote the single regressor as $X$ and the observed values of $X$ as $x_1, x_2, \\ldots, x_n$.\n",
        "\n",
        "For the height data, the 5 pairs of observed data are denoted\n",
        "\n",
        "$$\n",
        "(x_1, Y_1), (x_2, Y_2), \\ldots, (x_5, Y_5),\n",
        "$$\n",
        "\n",
        "with $(x_i, Y_i)$ denoting the data for observation $i$.\n",
        "\n",
        "-   $x_i$ denotes the mother’s height for observation $i$.\n",
        "-   $Y_i$ denotes the daughter’s height for observation $i$.\n",
        "\n",
        "**Selecting the best model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Suppose we want to find the straight line that best fits the plot of mother and daughter heights.\n",
        "\n",
        "How do we determine the “best fitting” model?\n",
        "\n",
        "Consider these potential “best fitting” lines that are drawn on the scatter plot of the height data. Which one is best?"
      ],
      "id": "fa53b026-2cfa-4bec-8aa9-195544fbe610"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyMjIya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\nYII=\n"
          }
        }
      ],
      "source": [],
      "id": "fb6ac958-7180-4907-8ee9-d0fd870bed11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Estimation of the simple linear regression model\n",
        "\n",
        "**Parameter estimation** is the process of using observed data to estimate model parameters.\n",
        "\n",
        "There are many different methods of parameter estimation in statistics:\n",
        "\n",
        "-   Method-of-moments.\n",
        "-   Maximum likelihood.\n",
        "-   Bayesian.\n",
        "-   Etc.\n",
        "\n",
        "The most common parameter estimation method for linear models is the **least squares method**, which is commonly called **Ordinary Least Squares (OLS)** estimation.\n",
        "\n",
        "OLS estimation estimates the regression coefficients with the values that minimize the residual sum of squares (RSS), which we will define shortly.\n",
        "\n",
        "**Defining a simple linear regression model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The regression model for $Y$ as a function of $X$, denoted $E(Y \\mid X)$, is the expected value of $Y$ conditional on the regressor $X$.\n",
        "\n",
        "The **simple linear regression model** for a response variable assumes the mean of $Y$ conditional on a single regressor $X$ is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The response variable $Y$ is modeled as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "where $\\epsilon$ is known as the model error.\n",
        "\n",
        "The error term $\\epsilon$ is literally the deviation of the response variable from its mean.\n",
        "\n",
        "-   We typically assume that conditional on the regressor variable, the error term has mean 0 and variance $\\sigma^2$.\n",
        "-   This is written as $E(\\epsilon \\mid X) = 0$ and $\\mathrm{var}(\\epsilon \\mid X) = \\sigma^2$.\n",
        "\n",
        "The observed data are modeled as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "for $i=1$, $2$,$\\ldots$,$n$, where $\\epsilon_i$ denotes the error for observation $i$.\n",
        "\n",
        "**Important terminology**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The **estimated regression model** or **fitted model** is defined as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "where $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the estimated value of our regression parameters.\n",
        "\n",
        "The $i\\text{th}$ **fitted value** is defined as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "-   The $i\\text{th}$ fitted value is the estimated mean of $Y$ when the regressor $X=x_i$.\n",
        "\n",
        "The $i\\text{th}$ **residual** is defined as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "-   The $i\\text{th}$ residual is the difference between the response and estimated mean response of observation $i$.\n",
        "\n",
        "The **residual sum of squares (RSS)** of a regression model is the sum of its squared residuals. The RSS for a simple linear regression model, as a function of the estimated regression coefficients $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, is defined as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Using the various objects defined above, there are many equivalent expressions for the RSS.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The **fitted model** is the estimated model that minimizes the RSS, and is written as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "-   $\\hat{Y}$ is used for brevity.\n",
        "-   $\\hat{E}(Y|X)$ is used for clarity.\n",
        "\n",
        "In a simple linear regression context, the fitted model is known as the **line of best fit**.\n",
        "\n",
        "**Visualizing terms**\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "15b68d8e-5ef6-4455-9c15-d179f8140eb3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP9NTU1oaGh8fHyM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\n"
          }
        }
      ],
      "source": [],
      "id": "18ba9840-30aa-4611-a3fa-f7026fea5f67"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the graphic above, we visualize the response values, fitted values, residuals, and fitted model in a simple linear regression context. Note that:\n",
        "\n",
        "-   The fitted model is shown as the dashed grey line and minimizes the RSS.\n",
        "-   The response values, shown as black dots, are the observed values of $Y$.\n",
        "-   The fitted values, shown as blue x’s, are the values returned by evaluating the fitted model at the observed regressor values.\n",
        "-   The residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative.\n",
        "-   The RSS is the sum of the squared vertical distances between the response and fitted values.\n",
        "\n",
        "**OLS estimators of the simple linear regression parameters**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The estimators of $\\beta_0$ and $\\beta_1$ that minimize the RSS for a simple linear regression model can be obtained analytically using basic calculus (as long as $x_1,\\ldots,x_n$ are not all equal to the same number).\n",
        "\n",
        "Define:\n",
        "\n",
        "-   $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$.\n",
        "-   $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n",
        "\n",
        "The OLS estimators of the simple linear regression coefficients that minimize the RSS are\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The most common estimator of the error variance is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "-   $\\mathrm{df}_{RSS}$ is the **degrees of freedom** of the RSS.\n",
        "-   For simple linear regression, $\\mathrm{df}_{RSS}=n-2$.\n",
        "\n",
        "# Penguins simple linear regression example\n",
        "\n",
        "The `penguins` data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman et al. (2014). We start by loading the data into memory."
      ],
      "id": "d0c7d1d9-558a-44ab-bbbb-80a2023a461a"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data(penguins, package = \"palmerpenguins\")\n",
        "head(penguins)"
      ],
      "id": "44d6b623-2f46-4a6d-be45-33e5e8c4b560"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data set includes 344 observations of 8 variables. The variables are:\n",
        "\n",
        "-   `species`: a `factor` indicating the penguin species.\n",
        "-   `island`: a `factor` indicating the island the penguin was observed.\n",
        "-   `bill_length_mm`: a `numeric` variable indicating the bill length in millimeters.\n",
        "-   `bill_depth_mm`: a `numeric` variable indicating the bill depth in millimeters.\n",
        "-   `flipper_length_mm`: an `integer` variable indicating the flipper length in millimeters\n",
        "-   `body_mass_g`: an `integer` variable indicating the body mass in grams.\n",
        "-   `sex`: a `factor` indicating the penguin sex (`female`, `male`).\n",
        "-   `year`: an integer denoting the study year the penguin was observed (`2007`, `2008`, or `2009`).\n",
        "\n",
        "We begin by creating a scatter plot of `bill_length_mm` versus `body_mass_g`."
      ],
      "id": "0d4c2d61-d250-4b84-89e4-cdfe80bab510"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(bill_length_mm ~ body_mass_g, data = penguins,\n",
        "     ylab = \"bill length (mm)\", xlab = \"body mass (g)\",\n",
        "     main = \"Penguin size measurements\")"
      ],
      "id": "eceddc4d-d196-42ce-9eaf-69a624fda6b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions:**\n",
        "\n",
        "-   Is there a positive/negative association between body mass and bill length?\n",
        "-   Is the relationship approximately linear?\n",
        "\n",
        "We will build a simple linear regression model that regresses `bill_length_mm` on `body_mass_g`.\n",
        "\n",
        "We want to estimate the parameters of the model\n",
        "\n",
        "$$\n",
        "E(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g})=\\beta_0+\\beta_1\\,\\mathtt{body\\_mass\\_g}.\n",
        "$$\n",
        "\n",
        "The `lm` function uses OLS estimation to fit a linear model to data. The function has two main arguments:\n",
        "\n",
        "-   `data`: the data frame in which the model variables are stored. This can be omitted if the variables are already stored in memory.\n",
        "-   `formula`: a Wilkinson and Rogers (1973) style formula describing the linear regression model. For complete details, run `?stats::formula` in the Console. If `y` is the response variable and `x` is an available numeric predictor, then `formula = y ~ x` tells `lm` to fit the simple linear regression model $E(Y|X)=\\beta_0+\\beta_1 X$."
      ],
      "id": "0fa383a5-ea05-4ee0-9eb5-1ce1c753c583"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "lmod <- lm(bill_length_mm ~ body_mass_g, data = penguins) # fit model\n",
        "class(lmod) # class of lmod"
      ],
      "id": "7d067396-2e74-4883-bc40-34bd31d29b99"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `summary` function is commonly used to summarize the results of our fitted model.\n",
        "\n",
        "When an `lm` object is supplied to the `summary` function, it returns:\n",
        "\n",
        "-   `Call`: the function call used to fit the model.\n",
        "-   `Residuals`: A 5-number summary of the $\\hat{\\epsilon}_1, \\ldots, \\hat{\\epsilon}_n$.\n",
        "-   `Coefficients`: A table that lists:\n",
        "    -   The regressors in the fitted model.\n",
        "    -   `Estimate`: the estimated coefficient for each regressor.\n",
        "    -   `Std. Error`: the *estimated* standard error of the estimated coefficients.\n",
        "    -   `t value`: the computed test statistic associated with testing $H_0: \\beta_j = 0$ versus $H_a: \\beta_j \\neq 0$ for each regression coefficient in the model.\n",
        "    -   `Pr(>|t|)`: the associated p-value of each test.\n",
        "-   Various summary statistics:\n",
        "    -   `Residual standard error` is the value of $\\hat{\\sigma}$, the estimate of the error standard deviation. The degrees of freedom is $\\mathrm{df}_{RSS}$, the number of observations minus the number of estimated coefficients in the model.\n",
        "    -   `Multiple R-squared` is an estimate of model fit.\n",
        "    -   `Adjusted R-squared` is a modified version of `Multiple R-squared`.\n",
        "    -   `F-statistic` is the test statistic for the test that compares the model with an only an intercept to the fitted model. The `DF` (degrees of freedom) values relate to the statistic under the null hypothesis, and the `p-value` is the p-value for the test.\n",
        "\n",
        "We use the `summary` function on `lmod` to produce the output below."
      ],
      "id": "80211cc5-9476-44e2-833c-0065afb8a70d"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize results stored in lmod\n",
        "summary(lmod)"
      ],
      "id": "055a34e5-6ccb-47d7-a79d-a8342f0dd61c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the output above, we see that the estimated parameters are $\\hat{\\beta}_0=26.9$ and $\\hat{\\beta}_1=0.004$.\n",
        "\n",
        "Our fitted model is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "In the context of a simple linear regression model:\n",
        "\n",
        "-   The intercept term is the expected response when the value of the regressor is zero.\n",
        "-   The slope is the expected change in the response when the regressor increases by 1 unit.\n",
        "\n",
        "Thus, based on the model we fit to the `penguins` data, we can make the following interpretations:\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Question:**\n",
        "\n",
        "-   Does the intercept term make sense physically?\n",
        "\n",
        "The `abline` function can be used to automatically overlay the fitted model on the observed data."
      ],
      "id": "a81e99e3-d7cd-455e-94d6-a9199d0555a1"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(bill_length_mm ~ body_mass_g,\n",
        "     data = penguins, main = \"Penguin size measurements\",\n",
        "     ylab = \"bill length (mm)\", xlab = \"body mass (g)\")\n",
        "# draw fitted line of plot\n",
        "abline(lmod)"
      ],
      "id": "baea1346-4f53-43be-bfa5-e6dda86845d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "R provides many additional methods (generic functions that do something specific when applied to a certain type of object) for `lm` objects. Commonly used ones include:\n",
        "\n",
        "We now use some of the methods to extract important characteristics of our fitted model.\n",
        "\n",
        "We extract the estimated regression coefficients, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, using the `coef` function."
      ],
      "id": "b5f5566f-ca8e-4daf-af77-0f641d6c7237"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "(coeffs <- coef(lmod)) # extract, assign, and print coefficients"
      ],
      "id": "b4472554-95c3-4429-8a28-1bfb8183b222"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the vector of residuals, $\\hat{\\epsilon}_1,\\ldots, \\hat{\\epsilon}_n$, using the `residuals` function."
      ],
      "id": "feee5a3a-18bb-44a1-aa54-5e87227084a4"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "ehat <- residuals(lmod) # extract and assign residuals\n",
        "head(ehat) # first few residuals"
      ],
      "id": "602b452f-3579-4910-9744-41cfda6cb547"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the vector of fitted values, $\\hat{Y}_1,\\ldots, \\hat{Y}_n$, using the `fitted` function."
      ],
      "id": "c3b9a2ca-7dfa-4937-a2bd-691cee0c5e81"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat <- fitted(lmod) # extract and assign fitted values\n",
        "head(yhat) # first few fitted values"
      ],
      "id": "864eed3a-b7d3-4bac-af48-46f240b7da5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also extract the vector of fitted values using the `predict` function."
      ],
      "id": "fcbff048-a111-459f-948b-535bfb4871ba"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat2 <- predict(lmod) # compute and assign fitted values\n",
        "head(yhat2) # first few fitted values"
      ],
      "id": "7430dd12-31a2-4a3a-a38c-ab0d000637f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the RSS of the model using the `deviance` function."
      ],
      "id": "569a5c96-8f0b-46c1-919c-6af579fe627e"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "(rss <- deviance(lmod)) # extract, assign, and print rss"
      ],
      "id": "1dbf195f-2284-4891-9bca-82cd7b863bd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the residual degrees of freedom using the `df.residual` function."
      ],
      "id": "259c81df-e456-4852-a1b2-95ff9c626320"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "(dfr <- df.residual(lmod)) # extract n - p"
      ],
      "id": "d70b4462-0260-4db2-98e4-061a2a867347"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract the estimated error standard deviation, $\\hat{\\sigma}=\\sqrt{\\hat{\\sigma}^2}$, using the `sigma` function. In the code below, we square $\\hat{\\sigma}$ to estimate the error variance, $\\hat{\\sigma}^2$."
      ],
      "id": "1e5d881f-febf-4a1f-aeeb-f58a2ee86135"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "(sigmasqhat <- sigma(lmod)^2) # estimated error variance"
      ],
      "id": "93ca8772-99a1-4354-8d48-cf8072dc0279"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:**\n",
        "\n",
        "-   What are the first three fitted values?\n",
        "-   What is the RSS of the fitted model?\n",
        "\n",
        "# Defining a linear model\n",
        "\n",
        "**Defining terms (again)**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "-   $Y$ denotes the response variable.\n",
        "    -   The response variable is treated as a random variable.\n",
        "    -   We will observe realizations of this random variable for each observation in our data set.\n",
        "-   $X$ denotes a single regressor variable. $X_1, X_2, \\ldots, X_{p-1}$ denote distinct regressor variables if we are performing regression with multiple regressor variables.\n",
        "    -   The regressor variables are treated as non-random variables.\n",
        "    -   The observed values of the regressor variables are treated as fixed, known values.\n",
        "-   $\\mathbb{X}=\\{X_0, X_1,\\ldots,X_{p-1}\\}$ denotes the collection of all regressors.\n",
        "    -   $X_0$ is usually the constant regressor 1, which is needed to include an intercept in the regression model.\n",
        "-   $\\beta_0$, $\\beta_1$, $\\ldots$, $\\beta_{p-1}$ denote **regression coefficients**.\n",
        "    -   Regression coefficients are statistical parameters that we will estimate from our data.\n",
        "    -   The regression coefficients are treated as fixed, non-random but unknown values.\n",
        "    -   Regression coefficients are not observable.\n",
        "-   $\\epsilon$ denotes model **error**.\n",
        "    -   The model error is more accurately described as random variation of each observation from the regression model.\n",
        "    -   The error is treated as a random variable.\n",
        "    -   The error is assumed to have mean 0 for all values of the regressors, i.e., $E(\\epsilon \\mid \\mathbb{X}) = 0$.\n",
        "    -   The variance of the errors is assumed to be a constant value for all values of the regressors, i.e.,$\\mathrm{var}(\\epsilon \\mid \\mathbb{X})=\\sigma^2$.\n",
        "    -   The error is never observable (except in the context of a simulation study where the experimenter literally defines the true model).\n",
        "\n",
        "**Standard definition of linear model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "In general, a linear regression model can have an arbitrary number of regressors.\n",
        "\n",
        "A **multiple linear regression** model has two or more regressors.\n",
        "\n",
        "A **linear model** for $Y$ is defined by the equation\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Notice:\n",
        "\n",
        "-   The response value equals the expected response for that combination of regressor values plus some error.\n",
        "-   $E(Y \\mid \\mathbb{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1}$.\n",
        "\n",
        "A linear regression model can be written as\n",
        "\n",
        "$$\n",
        "E(Y \\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} c_j \\beta_j.\n",
        "$$\n",
        "\n",
        "-   $c_0, c_1, \\ldots, c_{p-1}$ are known functions of the regressor variables.\n",
        "-   e.g., $c_1 = X_1 X_2 X_3$, $c_3 = X_2^2$, $c_8 = \\ln(X_1)/X_2^2$, etc.\n",
        "\n",
        "Alternatively, if $g_0,\\ldots,g_{p-1}$ are functions of $\\mathbb{X}$, then a linear regression model can be written as\n",
        "\n",
        "$$\n",
        "E(Y\\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} g_j(\\mathbb{X})\\beta_j.\n",
        "$$\n",
        "\n",
        "**Examples of a linear model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "A model is linear because of its *form* not the shape it produces.\n",
        "\n",
        "Some examples of linear regression models are:\n",
        "\n",
        "-   $E(Y|X) = \\beta_0$.\n",
        "-   $E(Y|X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2$.\n",
        "-   $E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
        "-   $E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$.\n",
        "-   $E(Y|X_1, X_2) = \\beta_0 + \\beta_1 \\ln(X_1) + \\beta_2 X_2^{-1}$.\n",
        "-   $E(\\ln(Y)|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
        "-   $E(Y^{-1}|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
        "\n",
        "Some examples of non-linear regression models are:\n",
        "\n",
        "-   $E(Y|X) = \\beta_0 + e^{\\beta_1 X}$.\n",
        "-   $E(Y|X) = \\beta_0 + \\beta_1 X/(\\beta_2 + X)$.\n",
        "\n",
        "# Estimation of the multiple linear regression model\n",
        "\n",
        "We want to estimate the parameters of the model\n",
        "\n",
        "$$\n",
        "Y=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon.\n",
        "$$\n",
        "\n",
        "The system of equations relating the responses, the regressors, and the errors for all $n$ observations can be written as\n",
        "\n",
        "$$\n",
        "Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n.\n",
        "$$\n",
        "\n",
        "**Using matrix notation to represent a linear model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We can simplify the linear model system of equations using using matrix notation.\n",
        "\n",
        "We use the following notation:\n",
        "\n",
        "-   $\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]$ denotes the column vector containing the $n$ observed response values.\n",
        "-   $\\mathbf{X}$ denotes the matrix containing a column of 1s and the observed regressor values for $X_1, X_2, \\ldots, X_{p-1}$. This may be written as\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "    1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p-1} \\\\\n",
        "    1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,p-1} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "    1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p-1}\n",
        "    \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "-   $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]$ denotes the column vector containing the $p$ regression coefficients.\n",
        "-   $\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]$ denotes the column vector contained the $n$ errors.\n",
        "\n",
        "The system of equations defining the linear model in can be written as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Matrix definitions of residuals, fitted values, and RSS\\* for multiple linear regression**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The vector of estimated values for the coefficients contained in $\\boldsymbol{\\beta}$ is denoted\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The vector of regressor values for the $i\\text{th}$ observation is denoted\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Question:**\n",
        "\n",
        "-   Why do we include a $1$ in our vector?\n",
        "\n",
        "The $i\\text{th}$ **fitted value** in the context of multiple linear regression is defined as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The notation “$\\mathbb{X} = \\mathbf{x}_i$” is a concise way of saying “$X_0 = 1, X_1=x_{i,1}, \\ldots, X_{p-1}=x_{i,p-1}$”.\n",
        "\n",
        "The column vector of fitted values is defined as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\mathbf{y}} &= [\\hat{Y}_1,\\ldots,\\hat{Y}_n] \\\\\n",
        "&= \\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The $i\\text{th}$ **residual** in the context of multiple linear regression can be written as\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The column vector of residuals is defined as $$\n",
        "\\hat{\\boldsymbol{\\epsilon}} = [\\hat{\\epsilon}_1,\\ldots,\\hat{\\epsilon}_n]. $$\n",
        "\n",
        "Equivalent expressions for the residual vector are\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The RSS for a multiple linear regression model, as a function of the estimated regression coefficients, is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**OLS estimator in matrix form**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The OLS estimator of the regression coefficient vector, $\\boldsymbol{\\beta}$, is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "This solution for $\\hat{\\boldsymbol{\\beta}}$ assumes $\\mathbf{X}$ has full-rank ($n>p$ and none of the columns of $\\mathbf{X}$ are linear combinations of other columns in $\\mathbf{X}$).\n",
        "\n",
        "The general estimator of the $\\sigma^2$ in the context of multiple linear regression is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "# Penguins multiple linear regression example\n",
        "\n",
        "We will fit a multiple linear regression model regressing `bill_length_mm` on `body_mass_g` and `flipper_length_mm`, and will once again do so using the `lm` function.\n",
        "\n",
        "**Formula notation**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Before we do that, we provide some additional discussion of the of the `formula` argument of the `lm` function. This will be very important as we discuss more complicated models. Assume `y` is the response variable and `x`, `x1`, `x2`, `x3` are available numeric predictors. Then:\n",
        "\n",
        "-   `y ~ x` describes the simple linear regression model $E(Y|X)=\\beta_0+\\beta_1 X$.\n",
        "-   `y ~ x1 + x2` describes the multiple linear regression model $E(Y|X_1, X_2)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2$.\n",
        "-   `y ~ x1 + x2 + x1:x2` and `y ~ x1 * x2` describe the multiple linear regression model $E(Y|X_1, X_2)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2$.\n",
        "-   `y ~ -1 + x1 + x2` describe a multiple linear regression model without an intercept, in this case, $E(Y|X_1, X_2)=\\beta_1 X_1 + \\beta_2 X_2$. The `-1` tells R not to include an intercept in the fitted model.\n",
        "-   `y ~ x + I(x^2)` describe the multiple linear regression model $E(Y|X)=\\beta_0+\\beta_1 X + \\beta_2 X^2$. The `I()` function is a special function that tells R to create a regressor based on the syntax inside the `()` and include that regressor in the model.\n",
        "\n",
        "**Fitting a model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We fit the linear model regressing `bill_length_mm` on `body_mass_g` and `flipper_length_mm` and extract some statistics."
      ],
      "id": "6e71e651-dde0-492b-8387-c61d0c41c4b4"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit model\n",
        "mlmod <- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, data = penguins)\n",
        "# extract estimated coefficients\n",
        "coef(mlmod)\n",
        "# extract RSS\n",
        "deviance(mlmod)"
      ],
      "id": "9d9de600-c0db-47cd-a7a7-60af8ddc488d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted model is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "-   **Question:** What is the RSS? How does it compare to the simple linear regression model we used before?\n",
        "\n",
        "# Types of linear models\n",
        "\n",
        "-   **Simple**: a model with an intercept and a single regressor.\n",
        "-   **Multiple**: a model with 2 or more regressors.\n",
        "-   **Polynomial**: a model with squared, cubic, quartic predictors, etc.\n",
        "    -   E.g, $E(Y\\mid X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3$ is a 4th-degree polynomial.\n",
        "-   **First-order**: a model in which each predictor is used to create no more than one regressor.\n",
        "-   **Main effect**: a model in which none of the regressors are functions of more than one predictor. A predictor can be used more than once, but each regressor is only a function of one predictor.\n",
        "    -   E.g., if $X_1$ and $X_2$ are different predictors, then the regression model $E(Y\\mid X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_2$ would be a main effect model, but not a first-order model since $X_1$ was used to create two regressors.\n",
        "-   **Interaction**: a model in which some of the regressors are functions of more than 1 predictor.\n",
        "    -   E.g., if $X_1$ and $X_2$ are different predictors, then the regression model $E(Y\\mid X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2$ is a very simple interaction model since the third regressor is the product of $X_1$ and $X_2$.\n",
        "-   **Analysis of variance (ANOVA)**: a model for which all predictors used in the model are categorical.\n",
        "-   **Analysis of covariance (ANCOVA)**: a model that uses at least one numeric predictor and at least one categorical predictor.\n",
        "-   **Generalized (GLM)**: a “generalized” linear regression model in which the responses do not come from a normal distribution.\n",
        "\n",
        "# Categorical predictors\n",
        "\n",
        "Categorical predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the variables.\n",
        "\n",
        "We discuss two basic linear regression models that have categorical predictors:\n",
        "\n",
        "-   **Parallel lines regression model**: a main effect regression model that has a single numeric regressor and a single categorical predictor.\n",
        "    -   The model produces parallel lines for each level of the categorical variable.\n",
        "-   **Separate lines regression model**, which adds an interaction term between the numeric regressor and categorical predictor of the parallel lines regression model.\n",
        "    -   The model produces separate lines for each level of the categorical variable.\n",
        "\n",
        "**Indicator variables**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "In order to compute $\\hat{\\boldsymbol{\\beta}}$ both $\\mathbf{X}$ and $\\mathbf{y}$ must contain numeric values.\n",
        "\n",
        "How can we use a categorical predictor in our regression model when its values are not numeric?\n",
        "\n",
        "We must transform the categorical predictor into one or more **indicator** or **dummy variables**.\n",
        "\n",
        "An **indicator function** is a function that takes the value 1 if a certain property is true and 0 otherwise.\n",
        "\n",
        "An **indicator variable** is the variable that results from applying an indicator function to each observation of a variable.\n",
        "\n",
        "We denote indicator functions using the notation,\n",
        "\n",
        "$$\n",
        "I_S(x) =\n",
        "\\begin{cases}\n",
        "1 & \\textrm{if}\\;x \\in S\\\\\n",
        "0 & \\textrm{if}\\;x \\notin S\n",
        "\\end{cases}.\n",
        "$$\n",
        "\n",
        "-   This function returns 1 if $x$ is in the set $S$ and 0 otherwise.\n",
        "\n",
        "Some examples of indicator functions:\n",
        "\n",
        "-   $I_{\\{2,3\\}}(2) = 1$.\n",
        "-   $I_{\\{2,3\\}}(2.5) = 1$.\n",
        "-   $I_{[2,3]}(2.5) = 1$, where $[2,3]$ is the interval from 2 to 3 and not the set containing only the numbers 2 and 3.\n",
        "-   $I_{\\{\\text{red},\\text{green}\\}}(\\text{green}) = 1$.\n",
        "\n",
        "Let $C$ denote a categorical predictor with levels $L_1$ and $L_2$.\n",
        "\n",
        "-   $C$ stands for “categorical”.\n",
        "-   $L$ stands for “level”.\n",
        "-   $c_i$ denotes the value of $C$ for observation $i$.\n",
        "\n",
        "Let $D_j$ denote the indicator (dummy) variable for factor level $L_j$ of $C$.\n",
        "\n",
        "-   The value of $D_j$ for observation $i$ is denoted $d_{i,j}$, with\n",
        "\n",
        "$$\n",
        "d_{i,j} = I_{\\{L_j\\}}(c_i).\n",
        "$$\n",
        "\n",
        "-   $d_{i,j}$ is 1 if $c_i$ has factor level $L_j$ and 0 otherwise.\n",
        "\n",
        "**Parallel and separate lines models**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Assume we want to build a linear regression model using a single numeric regressor $X$ and a two-level categorical predictor $C$.\n",
        "\n",
        "The standard simple linear regression model is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X)=\\beta_0 + \\beta_1 X.\n",
        "$$\n",
        "\n",
        "The parallel lines regression model is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X,C)=\\beta_{0}+\\beta_1 X+\\beta_2 D_2.\n",
        "$$\n",
        "\n",
        "Since $D_2=0$ when $C=L_1$ and $D_2=1$ when $C=L_2$,this model simplifies to\n",
        "\n",
        "$$\n",
        "E(Y\\mid X, C) =\n",
        "\\begin{cases}\n",
        "  \\beta_0+\\beta_1 X & \\mathrm{if}\\;C = L_1 \\\\\n",
        "  (\\beta_0 + \\beta_2) +\\beta_1 X & \\mathrm{if}\\;C = L_2.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "-   **Question:** What will the vertical distance between the lines be?\n",
        "\n",
        "The separate lines regression model is\n",
        "\n",
        "$$\n",
        "E(Y\\mid X,C)=\\beta_0+\\beta_1 X+\\beta_2 D_2 + \\beta_{3} XD_2.\n",
        "$$\n",
        "\n",
        "This model simplifies to\n",
        "\n",
        "$$\n",
        "E(Y\\mid X, C) =\n",
        "\\begin{cases}\n",
        "  \\beta_{0}+\\beta_1 X & \\mathrm{if}\\;C = L_1 \\\\\n",
        "  (\\beta_{0} + \\beta_{2}) +(\\beta_1 + \\beta_{3}) X & \\mathrm{if}\\;C = L_2.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**More complex models with categorical predictors**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "If you had a categorical predictor $C$ with $K$ levels $L_1, L_2, \\ldots, L_K$:\n",
        "\n",
        "-   We can add indicator variables $D_2, D_3, \\ldots, D_K$ to a simple linear regression model to create a parallel lines model for each level of $C$.\n",
        "-   We can add regressors $D_2, D_3, \\ldots, D_K, X D_2, X D_3, \\ldots, X D_K$ to a simple linear regression model to create a separate lines model for each level of $C$.\n",
        "\n",
        "It is easy to imagine using multiple categorical predictors in a model, interacting one or more categorical predictors with one or more numeric regressors in model, etc.\n",
        "\n",
        "These models can be fit easily using R but are more difficult to interpret.\n",
        "\n",
        "**Avoiding collinearity**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Consider the setting where $C$ has only 2 levels.\n",
        "\n",
        "Why don’t we add $D_1$ to the parallel lines model that already has $D_2$? Or $D_1$ and $D_1 X$ to the separate lines model that already has $D_2$ and $D_2 X$?\n",
        "\n",
        "-   We don’t *need* to add them.\n",
        "    -   If an observation doesn’t have level $L_2$ ($D_2=0$), then it must have level $L_1$.\n",
        "-   To avoid linear dependencies in the columns of the regressor matrix $\\mathbf{X}$!\n",
        "\n",
        "Consider a categorical variable $C$ with two only levels $L_1$ and $L_2$.\n",
        "\n",
        "-   Let $\\mathbf{d}_1=[d_{1,1}, d_{2,1}, \\ldots, d_{n,1}]$ denote the column vector of observed values for indicator variable $D_1$.\n",
        "-   Let $\\mathbf{d}_2$ be the column vector for $D_2$.\n",
        "-   Then $\\mathbf{d}_1 + \\mathbf{d}_2$ is an $n\\times 1$ vector of 1s.\n",
        "-   $D_1$ and $D_2$ will be linearly dependent with the intercept column of our $\\mathbf{X}$ matrix, which creates estimation problems.\n",
        "\n",
        "For a categorical predictor with $K$ levels, we only need indicator variables for $K-1$ levels of the categorical predictor.\n",
        "\n",
        "-   The level without an indicator variable in the regression model is known as the **reference level**.\n",
        "-   R automatically chooses the first level of a categorical (`factor`) variable to be the reference level, so we adopt that convention.\n",
        "\n",
        "# Penguins example with categorical predictor\n",
        "\n",
        "We display the grouped scatter plot of `bill_length_mm` versus `body_mass_g` that distinguishes the `species` of each observation."
      ],
      "id": "2101b83e-6bc4-488f-9bd0-06eff42b973e"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(ggplot2) # load package\n",
        "# create grouped scatterplot\n",
        "ggplot(data = penguins) +\n",
        "  geom_point(aes(x = body_mass_g, y = bill_length_mm, shape = species, color = species)) +\n",
        "  xlab(\"body mass (g)\") + ylab(\"bill length (mm)\")"
      ],
      "id": "02835ee1-2dc8-42be-86f3-83703e2f4261"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:**\n",
        "\n",
        "-   Does the relationship between bill length and body mass change depending on the species of penguin?\n",
        "\n",
        "**Using a categorical variable in the `lm` function**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "How do you use a categorical variable in R’s `lm` function?\n",
        "\n",
        "-   Each categorical variables should be a `factor`.\n",
        "-   The `lm` function will automatically convert a `factor` variable to the correct number of indicator variables when you include the `factor` variable in your `formula` argument.\n",
        "-   To add a main effect term for a categorical predictor, we simply add the term to our `lm` formula.\n",
        "-   To create an interaction term, we use `:` between the interacting variables.\n",
        "    -   E.g., if `c` is a `factor` variable and `x` is a `numeric` variable, you can use the notation `c:x` in your `formula` to get all the interactions between `c` and `x`.\n",
        "\n",
        "Our categorical predictor `species` has the levels `Adelie`, `Chinstrap`, and `Gentoo`.\n",
        "\n",
        "-   The first level of species is `Adelie`, so R will treat that level as the reference level.\n",
        "-   R will automatically create indicator variables for the levels `Chinstrap` and `Gentoo`.\n",
        "\n",
        "Let $D_C$ denote the indicator variable for the `Chinstrap` level and $D_G$ denote the indicator variable for the `Gentoo` level.\n",
        "\n",
        "**Fitting a parallel lines model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We fit the parallel lines regression model $$\n",
        "E(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) = \\beta_{0} + \\beta_1 \\mathtt{body\\_mass\\_g} + \\beta_2 D_C + \\beta_3 D_G.\n",
        "$$"
      ],
      "id": "2c0458d1-915c-42b3-940d-60e62f0e5691"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit parallel lines model\n",
        "lmodp <- lm(bill_length_mm ~ body_mass_g + species, data = penguins)\n",
        "# extract coefficients\n",
        "coef(lmodp)"
      ],
      "id": "b851d05d-da44-48b7-ba68-64b50c9ec3a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted parallel lines model is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Note that $D_C$ is `speciesChinstrap` and $D_G$ is `speciesGentoo`.\n",
        "\n",
        "When an observation has `species` level `Adelie`, then the model simplifies to\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "When an observation has `species` level `Chinstrap`, then the model simplifies to\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "When an observation has `species` level `Gentoo`, then the model simplifies to\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "We add our fitted lines for each `species` to our scatter plot.\n",
        "\n",
        "Let’s try adding our fitted values to the `penguins` data frame.\n",
        "\n",
        "-   We use the `predict` function to obtained the fitted values of our fitted model.\n",
        "-   We use the `transform` function to add those values as the `pl_fitted` variable in the `penguins` data frame."
      ],
      "id": "d3b2caf0-baa7-4dab-8d46-db1d6e21dbfd"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins <-\n",
        "  penguins |>\n",
        "  transform(pl_fitted = predict(lmodp))"
      ],
      "id": "ce3f3598-9336-4b15-898d-48414d182a85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   **Question:** Why are we getting this error?\n",
        "\n",
        "To handle this error, we refit our model while setting the `na.action` argument to `na.exclude`. As stated Details section of the documentation for the `lm` function (run `?lm` in the Console):\n",
        "\n",
        "> $\\ldots$ when `na.exclude` is used the residuals and predictions are padded to the correct length by inserting `NA`s for cases omitted by `na.exclude`.\n",
        "\n",
        "We refit the parallel lines model below with `na.action = na.exclude` and then repeat what we did before."
      ],
      "id": "c6de1de6-3987-4f20-be0b-cb36112d9423"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# refit parallel lines model with new na.action behavior\n",
        "lmodp <- lm(bill_length_mm ~ body_mass_g + species,\n",
        "            data = penguins, na.action = na.exclude)\n",
        "# add fitted values to penguins data frame\n",
        "penguins <-\n",
        "  penguins |>\n",
        "  transform(pl_fitted = predict(lmodp))"
      ],
      "id": "b103d2d8-aedf-4fba-874f-0c3d0feccae5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use the `geom_line` function to add the fitted lines for each `species` level to our scatter plot."
      ],
      "id": "91144f9c-566c-45a6-bd1d-c05e98bbf792"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create plot\n",
        "# create scatterplot\n",
        "# customize labels\n",
        "# add lines for each level of species\n",
        "ggplot(data = penguins) +\n",
        "  geom_point(aes(x = body_mass_g, y = bill_length_mm,\n",
        "                 shape = species, color = species)) +\n",
        "  xlab(\"body mass (g)\") + ylab(\"bill length (mm)\") +\n",
        "  geom_line(aes(x = body_mass_g, y = pl_fitted, color = species))"
      ],
      "id": "8751e681-48c5-42e5-bc1f-ccab2f989788"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Fitting a separate lines model**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We now fit the separate lines regression model\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&E(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) \\\\\n",
        "&= \\beta_{0} + \\beta_1 \\mathtt{body\\_mass\\_g} + \\beta_2 D_C + \\beta_3 D_G + \\beta_4 \\mathtt{body\\_mass\\_g} D_C + \\beta_5 \\mathtt{body\\_mass\\_g} D_G.\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "id": "2c343e55-d20d-4288-b443-9b20885cebdf"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit separate lines model\n",
        "# na.omit = na.exclude used to change predict behavior\n",
        "lmods <- lm(bill_length_mm ~ body_mass_g + species + body_mass_g:species,\n",
        "            data = penguins, na.action = na.exclude)\n",
        "# extract estimated coefficients\n",
        "coef(lmods)"
      ],
      "id": "7ba95888-ac08-44be-9fd7-e8652a002faa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted separate lines model is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "When an observation has `species` level `Adelie`, then the model simplifies to\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "When an observation has `species` level `Gentoo`, the model simplifies to\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "We use the code below to display the fitted lines for the separate lines model on the `penguins` data."
      ],
      "id": "c33b919d-bd86-4730-a9e9-95b9dc36fed4"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add separate lines fitted values to penguins data frame\n",
        "penguins <-\n",
        "  penguins |>\n",
        "  transform(sl_fitted = predict(lmods))\n",
        "# use geom_line to add fitted lines to plot\n",
        "ggplot(data = penguins) +\n",
        "  geom_point(aes(x = body_mass_g, y = bill_length_mm, shape = species, color = species)) +\n",
        "  xlab(\"body mass (g)\") + ylab(\"bill length (mm)\") +\n",
        "  geom_line(aes(x = body_mass_g, y = sl_fitted, col = species))"
      ],
      "id": "bd6106a9-744c-489a-81c9-f3f359d90fee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:**\n",
        "\n",
        "-   Do the fitted lines match the observed data behavior reasonably well?\n",
        "\n",
        "# Evaluating model fit\n",
        "\n",
        "**The coefficient of determination**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The most basic statistic measuring the fit of a regression model is the **coefficient of determination**, which is defined as\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2},\n",
        "$$\n",
        "\n",
        "where $\\bar{Y}$ is the sample mean of the observed response values.\n",
        "\n",
        "**Some sum-of-squares statistics**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "To interpret this statistic, we need to introduce some new “sum-of-squares” statistics similar to the RSS.\n",
        "\n",
        "The **total sum of squares** (corrected for the mean) is computed as\n",
        "\n",
        "$$\n",
        "TSS = \\sum_{i=1}^n(Y_i-\\bar{Y})^2.\n",
        "$$\n",
        "\n",
        "-   The TSS is the sum of the squared deviations of the response values from the sample mean.\n",
        "-   It has a more insightful interpretation.\n",
        "\n",
        "Consider the **constant mean model**, which is the model\n",
        "\n",
        "$$\n",
        "E(Y)=\\beta_0.\n",
        "$$\n",
        "\n",
        "Using basic calculus, we can show that the OLS estimator of $\\beta_0$ for the model in Equation @ref(eq:constant-mean-model) is $\\hat{\\beta}_0=\\bar{Y}$.\n",
        "\n",
        "For the constant mean model:\n",
        "\n",
        "-   $\\hat{Y}_i=\\hat{\\beta}_0$ for $i=1,2,\\ldots,n$.\n",
        "-   The RSS of the constant mean model is $\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2=\\sum_{i=1}^n(Y_i-\\bar{Y})^2$.\n",
        "\n",
        "*The TSS is the RSS for the constant mean model*.\n",
        "\n",
        "The **regression sum-of-squares** or **model sum-of-squares** is defined as\n",
        "\n",
        "$$\n",
        "SS_{reg} = \\sum_{i=1}^n(\\hat{Y}_i-\\bar{Y})^2.\n",
        "$$\n",
        "\n",
        "-   SS<sub>reg</sub> is the sum of the squared deviations between the fitted values of a model and the fitted values of the constant mean model.\n",
        "\n",
        "We have the following relationship between TSS, RSS, and SS<sub>reg</sub>:\n",
        "\n",
        "$$\n",
        "TSS = RSS + SS_{reg}.\n",
        "$$\n",
        "\n",
        "$SS_{reg}=TSS-RSS$.\n",
        "\n",
        "-   SS<sub>reg</sub> measures the reduction in RSS when comparing the fitted model to the constant mean model.\n",
        "\n",
        "**Equivalent expressiongs for $R^2$**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Some equivalent expressions for $R^2$ are\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The last expression is the squared sample correlation between the observed and fitted values, and is a helpful way to express the coefficient of determination because it extends to regression models that are not linear.\n",
        "\n",
        "*The coefficient of determination is the proportional reduction in RSS when comparing the fitted model to the constant mean model*.\n",
        "\n",
        "**Comments about the coefficient of determination**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "-   $0\\leq R^2 \\leq 1$.\n",
        "-   $R^2=0$ for the constant mean model.\n",
        "-   $R^2=1$ for a fitted model that perfectly fits the data (the fitted values match the observed response values).\n",
        "-   Generally, larger values of $R^2$ suggest that the model explains a lot of the variation in the response variable.\n",
        "-   Smaller $R^2$ values suggest the fitted model does not explain a lot of the response variation.\n",
        "-   The `Multiple R-squared` value printed by the `summary` of an `lm` object is $R^2$.\n",
        "-   To extract $R^2$ from a fitted model, you can use the syntax `summary(lmod)$r.squared`, where `lmod` is your fitted model.\n",
        "\n",
        "**Examples of $R^2$**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Consider the examples below relating $R^2$ to various fitted simple linear regression models.\n",
        "\n",
        "The coefficient of determination for the parallel lines model fit to the `penguins` data is 0.81."
      ],
      "id": "c3d3581c-2726-40d2-9d99-34d73e6db6d9"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(lmodp)$r.squared"
      ],
      "id": "55e48f5c-270b-4bff-8b1b-e92e8d757196"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By adding the `body_mass_g` regressor and `species` predictor to the constant mean model of `bill_length_mm`, we reduced the RSS by 81%.\n",
        "\n",
        "**Cautions about $R^2$**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "It is not wise to use $R^2$ to choose between models:\n",
        "\n",
        "-   $R^2$ never decreases as regressors are added to an existing model.\n",
        "    -   We can increase $R^2$ by simply adding regressors to your existing model, even if they are non-sensical.\n",
        "-   $R^2$ doesn’t tell you whether a model adequately describes the pattern of the observed data.\n",
        "    -   $R^2$ is a useful statistic for measuring model fit when there is approximately a linear relationship between the response values and fitted values.\n",
        "\n",
        "Let’s add some noise as a regressor to our parallel lines model."
      ],
      "id": "e49566ef-e118-4f24-a78d-8713bc4b8740"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(28) # for reproducibility\n",
        "# create regressor of random noise\n",
        "noisyx <- rnorm(344)\n",
        "# add noisyx as regressor to lmodp\n",
        "lmod_silly <- update(lmodp, . ~ . + noisyx)\n",
        "# extract R^2 from fitted model\n",
        "summary(lmod_silly)$r.squared"
      ],
      "id": "e4cdeea4-f881-42ce-8328-61453dce4b7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The $R^2$ value increased from 0.8080 to 0.8088!\n",
        "\n",
        "**Anscombe’s Quartet**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Anscombe (1973) provides a canonical data set known as “Anscombe’s quartet” that illustrates how $R^2$ can mislead you into thinking an inappropriate model fits better than it actually does.\n",
        "\n",
        "-   The data set is comprised of 4 different data sets.\n",
        "-   When a simple linear regression model is fit to each data set:\n",
        "    -   $\\hat{\\beta}_0=3$.\n",
        "    -   $\\hat{\\beta}_1=0.5$.\n",
        "    -   $R^2=0.67$.\n",
        "\n",
        "Anscombe’s quartet is available as the `anscombe` data set in the **datasets** package. The data set includes 11 observations of 8 variables. The variables are:\n",
        "\n",
        "-   `x1`, `x2`, `x3` `x4`: the regressor variable for each individual data set.\n",
        "-   `y1`, `y2`, `y3` `y4`: the response variable for each individual data set."
      ],
      "id": "2d29795d-acd7-4d14-9cd6-98f70509a748"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit model to first data set\n",
        "lmod_a1 <- lm(y1 ~ x1, data = anscombe)\n",
        "# extract coefficients from fitted model\n",
        "coef(lmod_a1)\n",
        "# extract R^2 from fitted model\n",
        "summary(lmod_a1)$r.squared\n",
        "# fit model to second data set\n",
        "lmod_a2 <- lm(y2 ~ x2, data = anscombe)\n",
        "coef(lmod_a2)\n",
        "summary(lmod_a2)$r.squared\n",
        "# fit model to third data set\n",
        "lmod_a3 <- lm(y3 ~ x3, data = anscombe)\n",
        "coef(lmod_a3)\n",
        "summary(lmod_a3)$r.squared\n",
        "# fit model to fourth data set\n",
        "lmod_a4 <- lm(y4 ~ x4, data = anscombe)\n",
        "coef(lmod_a4)\n",
        "summary(lmod_a4)$r.squared"
      ],
      "id": "ba2ff836-cab3-4c8b-b64e-b2206861e710"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We overlays the fitted models on each data set.\n",
        "\n",
        "Do all models describe the data equally well?\n",
        "\n",
        "**Adjusted R-squared**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Ezekiel (1930) proposed the adjusted R-squared statistic as a better statistic for measuring model fit. The adjusted $R^2$ statistic is defined as\n",
        "\n",
        "$$\n",
        "R^2_a=1-(1-R^2)\\frac{n-1}{n-p}=1-\\frac{RSS/(n-p)}{TSS/(n-1)}.\n",
        "$$\n",
        "\n",
        "-   $R^2_a$ will only increase when a regressors substantively improves the fit of the model to the observed data.\n",
        "-   We favor models with larger values of $R^2_a$.\n",
        "\n",
        "What is the $R^2_a$ for the 4 models we previously fit to the `penguins` data? Which is the “best” model?"
      ],
      "id": "8a82e820-f12c-40fc-bbb3-19ff0f89d81c"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simple linear regression model\n",
        "summary(lmod)$adj.r.squared\n",
        "# multiple linear regression model\n",
        "summary(mlmod)$adj.r.squared\n",
        "# parallel lines model\n",
        "summary(lmodp)$adj.r.squared\n",
        "# separate lines model\n",
        "summary(lmods)$adj.r.squared"
      ],
      "id": "3a0bdbbc-532a-4762-96ab-89193c0268d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We double-check that the separate lines model is a sensible model."
      ],
      "id": "234322c6-b3b4-41c1-91bf-d8b3a3818562"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(penguins$bill_length_mm ~ fitted(lmods),\n",
        "     xlab = \"fitted values\", ylab = \"bill length (mm)\")"
      ],
      "id": "05c9c99e-a912-438f-81ef-c9d92bbed21a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary of terms\n",
        "\n",
        "An overview of terms used to define a linear model.\n",
        "\n",
        "| Term                 | Description                                                                 | Observable? | Random? |\n",
        "|:--------------|:-------------------------------------------|:-------|:-----|\n",
        "| $Y$                  | response variable                                                           | Yes         | Yes     |\n",
        "| $Y_i$                | response value for the $i\\text{th}$ observation                             | Yes         | Yes     |\n",
        "| $\\mathbf{y}$         | the $n\\times 1$ column vector of response values                            | Yes         | Yes     |\n",
        "| $X$                  | regressor variable                                                          | Yes         | No      |\n",
        "| $X_j$                | the $j$th regressor variable                                                | Yes         | No      |\n",
        "| $x_{i,j}$            | the value of the $j$th regressor variable for the $i\\text{th}$ observation  | Yes         | No      |\n",
        "| $\\mathbf{X}$         | the $n\\times p$ matrix of regressor values                                  | Yes         | No      |\n",
        "| $\\mathbf{x}_i$       | the $p\\times 1$ vector of regressor values for the $i\\text{th}$ observation | Yes         | No      |\n",
        "| $\\beta_j$            | the coefficient associated with the $j$th regressor variable                | No          | No      |\n",
        "| $\\boldsymbol{\\beta}$ | the $p\\times 1$ column vector of regression coefficients                    | No          | No      |\n",
        "| $\\epsilon$           | the model error                                                             | No          | Yes     |\n",
        "| $\\epsilon_i$         | the error for the $i\\text{th}$ observation                                  | No          | Yes     |\n",
        "\n",
        "# Summary of functions\n",
        "\n",
        "An overview of important functions discussed in this chapter.\n",
        "\n",
        "| Function    | Purpose                                                                        |\n",
        "|:----------|:------------------------------------------------------------|\n",
        "| `lm`        | Fits a linear model based on a provided `formula`                              |\n",
        "| `summary`   | Provides summary information about the fitted model                            |\n",
        "| `coef`      | Extracts the vector of estimated regression coefficients from the fitted model |\n",
        "| `residuals` | Extracts the vector of residuals from the fitted model                         |\n",
        "| `fitted`    | Extracts the vector of fitted values from the fitted model                     |\n",
        "| `predict`   | Computes the fitted values (or arbitrary predictions) based on a fitted model  |\n",
        "| `deviance`  | Extracts the RSS of a fitted model                                             |\n",
        "| `sigma`     | Extracts $\\hat{\\sigma}$ from the fitted model                                  |\n",
        "| `update`    | Updates a fitted model to remove or add regressors                             |\n",
        "\n",
        "# Going Deeper\n",
        "\n",
        "## Derivation of the OLS estimators of the simple linear regression model coefficients\n",
        "\n",
        "Assume a simple linear regression model with $n$ observations.\n",
        "\n",
        "The residual sum of squares for the simple linear regression model is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**OLS estimator of $\\beta_0$**\n",
        "\n",
        "First, we take the partial derivative of the RSS with respect to $\\hat\\beta_0$ and simplify.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**OLS Estimator of $\\beta_1$**\n",
        "\n",
        "We start by taking the partial derivative of the RSS with respect to $\\hat{\\beta}_1$ and simplify.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "## Unbiasedness of OLS estimators\n",
        "\n",
        "We now show that the OLS estimators of the simple linear regression coefficients are unbiased.\n",
        "\n",
        "An estimator is unbiased if the expected value is equal to the parameter it is estimating.\n",
        "\n",
        "We want to show that\n",
        "\n",
        "$$\n",
        "E(\\hat{\\beta}_0\\mid \\mathbf{X})=\\beta_0.\n",
        "$$\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "## Derivation of the OLS estimator for the multiple linear regression model coefficients\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "# References\n",
        "\n",
        "Anscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\n",
        "\n",
        "Ezekiel, Mordecai. 1930. “Methods of Correlation Analysis.”\n",
        "\n",
        "Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. <https://doi.org/10.1371/journal.pone.0090081>.\n",
        "\n",
        "Weisberg, Sanford. 2014. Applied Linear Regression. Fourth. Hoboken NJ: Wiley. <http://z.umn.edu/alr4ed>.\n",
        "\n",
        "Wilkinson, GN, and CE Rogers. 1973. “Symbolic Description of Factorial Models for Analysis of Variance.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 22 (3): 392–99."
      ],
      "id": "5795693a-d0c9-4915-b92b-eb4f6fa895ce"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "pygments_lexer": "r",
      "version": "4.2.2"
    }
  }
}