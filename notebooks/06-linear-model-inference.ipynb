{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6 - Linear Model Inference\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click the Open in Colab graphic below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/06-linear-model-inference-notebook.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"> </a>\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "b632f44e-8ca0-4a1d-a6ee-344ec5b5c122"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(!require(palmerpenguins, quietly = TRUE)) {\n",
        "  install.packages(\"palmerpenguins\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(palmerpenguins)\n",
        "}\n",
        "if(!require(api2lm, quietly = TRUE)) {\n",
        "  install.packages(\"apli2m\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(api2lm)\n",
        "}"
      ],
      "id": "992cd807-1f09-4add-8508-c86a4b1599c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview of inference and prediction\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Inference and prediction are often intertwined, so we discuss them together.\n",
        "\n",
        "*Inference* focuses on drawing conclusions about the data-generating distribution\n",
        "\n",
        "-   The two primary types of statistical inference are confidence intervals and hypothesis tests.\n",
        "-   The Estimation & Inference Appendix provides an overview of both confidence intervals and hypothesis tests in a more general context.\n",
        "\n",
        "*Prediction* focuses on selecting a plausible value or range of values for an unobserved response.\n",
        "\n",
        "-   Prediction often makes use of estimated parameters we find as part of the inferential process, though this isn’t required.\n",
        "\n",
        "We will also introduce and discuss solutions for the multiple comparisons problem, which arises when we make multiple inferences or predictions simultaneously.\n",
        "\n",
        "# Necessary notation\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The $t$ distribution is a symmetric bell-shaped distribution like the normal distribution but has a larger standard deviation.\n",
        "\n",
        "-   The $t$ distribution has a degrees of freedom parameter, $\\nu$.\n",
        "-   $t_{\\nu}$ denotes a random variable having a $t$ distribution with $\\nu$ degrees of freedom.\n",
        "    -   As the degrees of freedom of a $t$ random variable increases it behaves more and more similarly to a random variable with a standard normal distribution (a $\\mathsf{N}(0,1)$ distribution).\n",
        "-   $t_{\\nu}^{\\alpha}$ denotes the $1-\\alpha$ quantile of a $t$ distribution with $\\nu$ degrees of freedom.\n",
        "\n",
        "The figure below displays the density of a $t$ distribution with 10 degrees of freedom while also indicating the 0.95 quantile of that distribution.\n",
        "\n",
        "-   Additional information about the $t$ distribution is available on [Wikipedia](https://en.wikipedia.org/wiki/Student%27s_t-distribution)."
      ],
      "id": "d8f12c90-691c-49d4-b811-5a977fa4f6a2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyMjIya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\nRK5CYII=\n"
          }
        }
      ],
      "source": [],
      "id": "41167490-7cdf-4a86-8682-3dab46fb914f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The $F$ distribution is characterized as the ratio of two independent chi-square random variables divided by their degrees of freedom.\n",
        "\n",
        "-   It has a numerator degrees of freedom parameter, $\\nu_1$, and a denominator degrees of freedom parameter, $\\nu_2$.\n",
        "-   The notation $F_{\\nu_1, \\nu_2}$ denotes a random variable having an $F$ distribution with $\\nu_1$ numerator degrees of freedom and $\\nu_2$ denominator degrees of freedom.\n",
        "-   $F^{\\alpha}_{\\nu_1,\\nu_2}$ denotes the $1-\\alpha$ quantile of an $F$ random variables with $\\nu_1$ numerator degrees of freedom and $\\nu_2$ denominator degrees of freedom.\n",
        "-   In fact, $[t_{\\nu}]^2=F_{1,\\nu}$, i.e., the square of a $t$ random variable with $\\nu$ degrees of freedom is equivalent to an $F$ random variable with $1$ numerator degree of freedom and $\\nu$ denominator degrees of freedom.\n",
        "\n",
        "# Properties of the OLS estimator\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We assume:\n",
        "\n",
        "-   $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}$.\n",
        "\n",
        "-   $\\boldsymbol{\\epsilon}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{0}_{n\\times 1},\\sigma^2 \\mathbf{I}_{n\\times n})$.\n",
        "\n",
        "This assumption applies to all errors, so we believe that all errors, observed and future, will have mean 0, variance $\\sigma^2$, will be uncorrelated with other observations, and have a normal distribution.\n",
        "\n",
        "Under these assumptions, we showed before that\n",
        "\n",
        "-   $\\mathbf{y}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_{n\\times n})$.\n",
        "-   $\\hat{\\boldsymbol{\\beta}}\\mid \\mathbf{X} \\sim \\mathsf{N}(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1})$.\n",
        "\n",
        "# Parametric confidence intervals for regression coefficients\n",
        "\n",
        "**Standard $t$-based confidence intervals**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Under the assumptions above, we can prove (though we won’t) that\n",
        "\n",
        "$$\n",
        "\\frac{\\hat{\\beta}_j-\\beta_j}{\\hat{\\mathrm{se}}(\\hat{\\beta}_j)}\\sim t_{n-p}, \\quad j=0,1,\\ldots,p-1,\n",
        "$$\n",
        "\n",
        "where the estimated standard error of $\\hat{\\beta}_j$ is\n",
        "\n",
        "$$\n",
        "\\hat{\\mathrm{se}}(\\hat{\\beta}_j)=\\hat{\\sigma}(\\mathbf{X}^T\\mathbf{X})^{-1}_{j+1,j+1}.\n",
        "$$\n",
        "\n",
        "Recall that:\n",
        "\n",
        "-   Estimated standard error is the estimated standard deviation of the sampling distribution of $\\hat{\\beta}_j$.\n",
        "-   The notation $(\\mathbf{X}^T\\mathbf{X})^{-1}_{j+1,j+1}$ indicates the element in row $j+1$, column $j+1$, of the matrix $(\\mathbf{X}^T\\mathbf{X})^{-1}$.\n",
        "\n",
        "$(\\hat{\\beta}_j-\\beta_j)/\\hat{\\mathrm{se}}(\\hat{\\beta}_j)$ is a pivotal quantity with a $t$ distribution, and it can be used to derive a confidence interval.\n",
        "\n",
        "A confidence interval for $\\beta_j$ with confidence level $1-\\alpha$ is given by the expression:\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_j \\pm t^{\\alpha/2}_{n-p} \\hat{\\mathrm{se}}(\\hat{\\beta}_j),\\quad j=0,2,\\ldots,p-1.\n",
        "$$\n",
        "\n",
        "It is critical to note that the $1-\\alpha$ confidence level refers to the procedure for a single interval, not the family of intervals we can produce for all $p$ coefficients.\n",
        "\n",
        "The `confint` function returns confidence intervals for the regression coefficients of a fitted model.\n",
        "\n",
        "The `confint` function has 3 main arguments:\n",
        "\n",
        "-   `object`: a fitted model (`lm`) object.\n",
        "-   `parm`: a vector of numbers or names indicating the parameters for which we want to construct confidence intervals.\n",
        "    -   By default, confidence intervals are constructed for all parameters.\n",
        "-   `level`: the confidence level desired for the confidence interval.\n",
        "    -   The default value is `0.95`, which will produce 95% confidence intervals.\n",
        "\n",
        "We once again use the `penguins` data from the **palmerpenguins** package to illustrate what we have learned.\n",
        "\n",
        "**Penguins example**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Consider the regression model:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&E(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g}, \\mathtt{flipper\\_length\\_mm}) \\\\\n",
        "&=\\beta_0+\\beta_1 \\mathtt{body\\_mass\\_g} + \\beta_2 \\mathtt{flipper\\_length\\_mm}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We estimate the parameters of this model in R."
      ],
      "id": "9a3b1270-1059-4a7f-92bb-d2df3549675a"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "data(penguins, package = \"palmerpenguins\")\n",
        "# fit model\n",
        "mlmod <- lm(bill_length_mm ~ body_mass_g + flipper_length_mm,\n",
        "            data = penguins)"
      ],
      "id": "7d3358e7-21f2-496a-8de4-6abe5a256e00"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain the 95% confidence intervals for the 3 regression coefficients."
      ],
      "id": "a3abced5-7aad-4769-b200-021bcab225b5"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "confint(mlmod)"
      ],
      "id": "58cb2f34-fe25-412c-9a44-b32e5dc85c23"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 95% confidence interval for the intercept parameter is \\[-12.45, 5.58\\]. We are 95% confident that the mean penguin bill length is between -12.25 and 5.58 mm for a penguin with a body mass of 0 g and a flipper length of 0 mm. (This really isn’t sensible).\n",
        "\n",
        "The 95% confidence interval for the `body_mass_g` coefficient is \\[-0.00046, 0.002\\]. We are 95% confident the regression coefficient for `body_mass_g` is between -0.00046 and 0.002, assuming the `flipper_length_mm` regressor is also in the model.\n",
        "\n",
        "If we wanted to get the 90% confidence interval for the `flipper_length_mm` coefficient by itself, we could use either of the commands shown below."
      ],
      "id": "1c4a411b-7c2c-41fa-be2b-a484c6c8b4e9"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# two styles for determining the CI for a single parameter (at a 90% level)\n",
        "confint(mlmod, parm = 3, level = 0.90)\n",
        "confint(mlmod, parm = \"flipper_length_mm\", level = 0.90)"
      ],
      "id": "c9464cd6-5d08-4ad0-a933-3e86a00e0826"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The multiple comparisons problem\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Our linear models typically have multiple regression coefficients.\n",
        "\n",
        "-   We typically want to construct confidence intervals for all of the coefficients.\n",
        "\n",
        "The confidence level of the procedure described above is only valid for a single interval.\n",
        "\n",
        "Since we are constructing multiple intervals, the simultaneous confidence level of the procedure for the family of intervals is less than $1-\\alpha$.\n",
        "\n",
        "This is an example of the multiple comparisons problem.\n",
        "\n",
        "A **multiple comparisons problem** occurs anytime we make multiple inferences (confidence intervals, hypothesis tests, prediction intervals, etc.).\n",
        "\n",
        "-   We are more likely to draw erroneous conclusions if we do not adjust for the fact that we are making multiple inferential statements.\n",
        "-   E.g., a confidence interval procedure with level 0.95 will produce intervals that contain the target parameter with probability 0.95.\n",
        "-   If we construct two confidence intervals with level 0.95, then the family-wise confidence level (i.e., the probability that both intervals simultaneously contain their respective target parameters) will be less than 0.95.\n",
        "-   We can guarantee that our family-wise confidence level will be at least 0.90, but we can’t determine the exact value without more information\n",
        "\n",
        "The **family-wise confidence level** is the probability that all intervals under consideration simultaneously contain their target parameter.\n",
        "\n",
        "-   The family-wise confidence level is also known as the **simultaneous** or **overall** confidence level.\n",
        "\n",
        "A **multiple comparisons procedure** is a procedure designed to adjust for multiple inferences.\n",
        "\n",
        "-   E.g., a multiple comparisons procedure will produce a family of intervals that have a family-wise confidence level above some threshold.\n",
        "\n",
        "**Adjusted confidence intervals for regression coefficients**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Bonferroni (1936) proposed a simple multiple comparisons procedure that is applicable in many contexts. This general procedure is known as the **Bonferroni correction**.\n",
        "\n",
        "Suppose we are constructing $k$ confidence intervals simultaneously. We control the family-wise confidence level of our intervals at $1-\\alpha$ if we construct the individual confidence intervals with the level $1-\\alpha/k$. We sketch a proof of this below.\n",
        "\n",
        "Boole’s inequality (Boole, 1847) states that for a countable set of events $A_1, A_2, A_3 \\ldots$,\n",
        "\n",
        "$$P(\\cup_{j=1}^\\infty A_j) \\leq \\sum_{j=1}^\\infty P(A_j).$$\n",
        "\n",
        "This is a generalization of the fact that\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A\\cap B) \\leq P(A) + P(B).\n",
        "$$\n",
        "\n",
        "for two events $A$ and $B$.\n",
        "\n",
        "Suppose that we construct a family of $k$ confidence intervals with individual confidence level $1-\\alpha/k$ (and all assumptions are satisfied.)\n",
        "\n",
        "-   The probability that the confidence interval procedure for a specific interval doesn’t contain the target parameter is $\\alpha/k$.\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& P(\\mbox{All }k\\mbox{ intervals contain the target parameter}) \\\\\n",
        "& = 1 - P(\\mbox{At least one of the }k\\mbox{ intervals misses the target parameter}) \\\\\n",
        "& = 1 - P(\\cup_{j=1}^k \\mbox{interval }j\\mbox{ misses the target parameter}) \\\\\n",
        "& \\geq 1 - \\sum_{j=1}^k P(\\mbox{interval }j\\mbox{ misses the target parameter}) \\\\\n",
        "& = 1 - k(\\alpha/k) \\\\\n",
        "&= 1-\\alpha.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus, the family-wise confidence level of all $k$ intervals is AT LEAST $1-\\alpha$ when the Bonferroni correction is used.\n",
        "\n",
        "The Bonferroni correction is known to be conservative, which means that the family-wise confidence level is typically much larger than $1-\\alpha$.\n",
        "\n",
        "-   Conservative methods can have low power.\n",
        "-   i.e., Our intervals are much wider than they need to be, so we aren’t able to draw precise conclusions about the plausible values of our regression coefficients.\n",
        "\n",
        "**Penguins example (continued)**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Let’s construct simultaneous confidence intervals for our `penguins` example using the Bonferroni correction.\n",
        "\n",
        "If we want to control the family-wise confidence level of our $k=3$ intervals at $0.95$, then $\\alpha = 0.05$ and the Bonferroni correction suggests that we should construct the individual intervals at a confidence level of $1-0.05/3=0.983$.\n",
        "\n",
        "We construct the Bonferroni-adjusted confidence intervals using the code below."
      ],
      "id": "2c65206f-8954-40b8-abf9-ee26b0bff593"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simultaneous 95% confidence intervals for mlmod\n",
        "confint(mlmod, level = 1 - 0.05/3)"
      ],
      "id": "3cf9e5fe-d2a1-4877-a25e-8b96c1035093"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we can use the `confint_adjust` function from the **api2lm** package to construct this interval.\n",
        "\n",
        "The `confint_adjust` function works almost identically to the `confint` function.\n",
        "\n",
        "-   The `confint_adjust` function takes a `method` argument to indicate the type of adjustment to make when constructing the confidence intervals.\n",
        "-   e.g., `method = \"bonferroni\"` will produce Bonferroni-corrected intervals."
      ],
      "id": "f8c67c5a-780a-4133-ad80-216820dd49b5"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "confint_adjust(mlmod, method = \"bonferroni\")"
      ],
      "id": "c1886bb2-c41b-45fe-afdb-6e9c9525456f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working and Hotelling (1929) developed another multiple comparisons procedure that can be used to preserve the family-wise confidence level of the intervals at $1-\\alpha$.\n",
        "\n",
        "The Working-Hotelling multiple comparisons procedure is valid for ALL linear combinations of the regression coefficients, meaning that we can construct an arbitrarily large number of confidence intervals for linear combinations of the regression coefficients with this procedure and the family-wise confidence level will be at least $1-\\alpha$ (Weisberg, 2014).\n",
        "\n",
        "The Working-Hotelling procedure guarantees that if we construct the individual confidence intervals in the following way, then the family-wise confidence level will be at least $1-\\alpha$:\n",
        "\n",
        "$$\n",
        "\\hat{\\beta}_j \\pm \\sqrt{p F^\\alpha_{p,n-p}} \\hat{\\mathrm{se}}(\\hat{\\beta}_j),\\quad j=0,2,\\ldots,p-1.\n",
        "$$\n",
        "\n",
        "The `confint_adjust` function from the **api2lm** package will produce these intervals when setting the `method` argument to `\"wh\"`. We construct Working-Hotelling-adjusted intervals with family-wise confidence level of at least 0.95 for the `penguins` example using the code below."
      ],
      "id": "57463ac6-6bfb-4ae6-b2cd-843a6f75bbd4"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "confint_adjust(mlmod, method = \"wh\")"
      ],
      "id": "9fc6bcfa-6757-4ce9-8cc1-2e2e8af9e273"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, the Bonferroni-adjusted intervals are narrower than the Working-Hotelling-adjusted intervals.\n",
        "\n",
        "The Working-Hotelling intervals tends to be narrower for small $p$ (e.g., $p=1$ or $2$) and small $n-p$ (e.g., $n-p = 1$ or $2$) (Mi and Sampson, 1993).\n",
        "\n",
        "As the number of intervals increases, the Working-Hotelling intervals will eventually be narrower than the Bonferroni-adjusted intervals.\n",
        "\n",
        "# Prediction: mean response versus new response\n",
        "\n",
        "It is common to make two types of predictions in a regression context:\n",
        "\n",
        "-   Prediction of a mean response.\n",
        "-   Prediction of a new response.\n",
        "\n",
        "In either context, we want to make predictions with respect to a specific combination of regressor values, which we denote $\\mathbf{x}_0$.\n",
        "\n",
        "The mean response for a specific combination of regressors is denoted $E(Y\\mid \\mathbb{X}=\\mathbf{x}_0)$.\n",
        "\n",
        "We use the notation $Y(\\mathbf{x}_0)$ to describe a new response for a specific combination of regressor values, $\\mathbb{X}=\\mathbb{x_0}$.\n",
        "\n",
        "$E(Y\\mid \\mathbb{X}=\\mathbf{x}_0)$ represents the average response when the regressor values are $\\mathbf{x}_0$.\n",
        "\n",
        "-   i.e., the number we would get if were able to determine the average of an infinite number of responses with regressor values being fixed at $\\mathbf{x}_0$.\n",
        "\n",
        "$Y(\\mathbf{x}_0)$ represents the actual response we will will observe for a new observation with regressor values $\\mathbf{x}_0$.\n",
        "\n",
        "-   $Y(\\mathbf{x}_0)$ represents the mean response for that combination of regressor values plus some error.\n",
        "-   $Y(\\mathbf{x}_0)=E(Y\\mid \\mathbb{X}=\\mathbf{x}_0)+\\epsilon(\\mathbf{x}_0)$, where $\\epsilon(\\mathbf{x_0})$ denotes the error for our new observation.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we want to rent a new apartment or buy a new house. If we look through the available listings, we will likely filter our search results by certain characteristics. We might limit our search to dwellings with 3 bedrooms, 2 bathrooms, that are within a certain distance of public transportation, and have a certain amount of square footage.\n",
        "\n",
        "-   If we averaged the monthly rent or asking price of all the dwellings matching our specifications, then that would be an approximation of the mean response (we would need all the possible dwellings matching those considerations to get the true average).\n",
        "-   This average would give us an idea of the “typical” price of dwellings with those characteristics.\n",
        "-   On the other hand, we likely want to know the price of the dwelling we actually end up in. This is the “new response” we want to predict.\n",
        "\n",
        "It is common to distinguish the two scenarios by using the terminology:\n",
        "\n",
        "-   “Estimating the mean response” to refer to prediction of the the mean\n",
        "-   “Prediction a new response” when we want to predict a new observation.\n",
        "\n",
        "# Confidence interval for the mean response\n",
        "\n",
        "Consider a typical linear regression model with $p$ regression coefficients given by:\n",
        "\n",
        "$$\n",
        "E(Y|\\mathbb{X})=\\beta_0+\\beta_1 X_1 + \\ldots \\beta_{p-1} X_{p-1}\n",
        "$$\n",
        "\n",
        "We want to estimate the mean response for a specific combination of regressor values. The mean response for that combination of regressors is obtained via the equivalent expressions\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E(Y\\mid \\mathbb{X}=\\mathbf{x}_0) &= \\beta_0 + \\sum_{j=1}^{p-1}x_{0,j}\\beta_j \\\\\n",
        "&= \\mathbf{x}_0^T \\boldsymbol{\\beta}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To simplify our notation, we drop the “$\\mathbb{X}=$” in our discussion below, so $E(Y\\mid \\mathbb{X}=\\mathbf{x}_0)\\equiv E(Y\\mid \\mathbf{x}_0)$.\n",
        "\n",
        "What does $E(Y\\mid \\mathbf{x}_0)$ represent? It represents the average response we will observe if we somehow managed to observe infinitely many responses with $\\mathbb{X}=\\mathbf{x}_0$.\n",
        "\n",
        "The Gauss-Markov Theorem discussed before indicates that the best linear unbiased estimator of the mean response is given by the equation:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{E}(Y\\mid \\mathbf{x}_0) &= \\hat{\\beta}_0 + \\sum_{j=1}^{p-1}x_{0,j}\\hat{\\beta}_j \\\\\n",
        "&= \\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "which replaces the unknown, true coefficients by their OLS estimates.\n",
        "\n",
        "We want to create a confidence interval for $E(Y\\mid \\mathbf{x}_0)$. If we divide the estimation error of the mean response, i.e., $E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)$, by its estimated standard deviation, then we obtain a pivotal quantity. More specifically, we have:\n",
        "\n",
        "$$\n",
        "\\frac{E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)}{\\sqrt{\\hat{\\mathrm{var}}\\left(E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)\\right)}} = \\frac{E(Y\\mid \\mathbf{x}_0)- \\mathbf{x}_0 \\hat{\\boldsymbol{\\beta}}}{ \\hat{\\mathrm{se}}(\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}})}\\sim t_{n-p}\n",
        "$$\n",
        "\n",
        "with:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathrm{se}}(\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}})=\\hat{\\sigma}\\sqrt{\\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0}. (\\#eq:sehat-est-mean)\n",
        "$$\n",
        "\n",
        "A confidence interval for $E(Y\\mid \\mathbf{x}_0)$ with confidence level $1-\\alpha$ is given by the expression:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}} \\pm t^{\\alpha/2}_{n-p} \\hat{\\mathrm{se}}(\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}}). (\\#eq:t-ci-mean-response)\n",
        "$$\n",
        "\n",
        "The `predict` function is a generic function used to make predictions based on fitted models. We can use this function to estimate the mean response for multiple combinations of predictor variables, compute the estimated standard error of each estimate, and obtain confidence intervals for the mean response. The primary arguments to the `lm` method for `predict` are:\n",
        "\n",
        "-   `object`: A fitted model from the `lm` function.\n",
        "-   `newdata`: A data frame of predictor values. All predictors used in the formula used to fit `object` must be provided. Each row contains the predictor values for the mean response we want to estimate. If this is not provided, then the fitted values for each observation are returned.\n",
        "-   `se.fit`: A logical value indicating whether we want to explicitly compute the standard errors of each estimated mean, i.e., $\\hat{\\mathrm{se}}(\\hat{E}(Y\\mid \\mathbf{x}_0))$ for each estimate.\n",
        "-   `interval`: The type of interval to compute. The default is `none`, meaning no interval is provided. Setting `interval = \"confidence\"` will return a confidence interval for the mean response associated with each row of `newdata`. Setting `interval = \"prediction\"` will return a prediction interval for a new response, which we will discuss in the next section.\n",
        "-   `level`: The confidence level of the interval.\n",
        "\n",
        "Run `?predict.lm` in the Console for additional details about this function.\n",
        "\n",
        "We will estimate the mean response for the parallel lines model previously fit to the `penguins` data. We do this to emphasize the fact that the `predict` function asks you to specify the values of the *predictor* variables for each estimate you want to make, not the complete set of regressors.\n",
        "\n",
        "Recall we fit a parallel lines model to the `penguins` data that used both `body_mass_g` and `species` to explain the behavior of `bill_length_mm`. Letting $D_C$ denote the indicator variable for the `Chinstrap` level and $D_G$ denote the indicator variable for the `Gentoo` level, the fitted parallel lines model was:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species})\\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 D_C + 3.56 D_G.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We fit this model below, assigning it the name `lmodp`."
      ],
      "id": "f567a024-c227-430d-be15-b81bd6c553da"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit parallel lines model to penguins data\n",
        "lmodp <- lm(bill_length_mm ~ body_mass_g + species,\n",
        "            data = penguins)\n",
        "coef(lmodp)"
      ],
      "id": "55624b89-db5e-4949-8faa-7ab432a9e5b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s estimate the mean response for the “typical” `body_mass_g` of each `species`. We compute the mean `body_mass_g` of each `species` using the code below, assigning the resulting data frame the name `newpenguins`. We use the `group_by` and `summarize` functions from the **dplyr** package \\[@R-dplyr\\] to simplify this process."
      ],
      "id": "639218de-e650-438d-b874-ac80fc03efbe"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean body_mass_g of each species\n",
        "newpenguins <- penguins |>\n",
        "  dplyr::group_by(species) |>\n",
        "  dplyr::summarize(body_mass_g = mean(body_mass_g, na.rm = TRUE))\n",
        "newpenguins"
      ],
      "id": "d37724a0-da43-4ddf-a25c-4a5fd3b1d7c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have a data frame with variables for the two predictors, `species` and `body_mass_g` , used to fit `lmodp`. Each row of `newpenguins` contains the mean `body_mass_g` for each level of `species` and is suitable for use in the `predict` function.\n",
        "\n",
        "In the code below, we estimate the mean `flipper_length_mm` based on the fitted model in `lmodp` for the mean `body_mass_g` of each level of `species`. We also choose to compute the estimated standard errors of each estimate by setting the `se.fit` argument to `TRUE`."
      ],
      "id": "828e6ecd-5eb7-45db-abc6-c45f5501d4ff"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimate mean and standard error for 3 combinations of predictors\n",
        "predict(lmodp, newdata = newpenguins, se.fit = TRUE)"
      ],
      "id": "1d854d6c-5b34-4b52-ba5e-f3018470f3f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An Adelie penguin with a body mass of 3700.662 g is estimated to have a mean flipper length of 38.79 mm with an estimated standard error of 0.196 mm. A Chinstrap penguin with a body mass of 3733.09 g is estimated to have a mean flipper length of 48.83 mm with an estimated standard error of 0.29 mm. A Gentoo penguin with a body mass of 5076.02 g is estimated to have a mean flipper length of 47.50 mm with an estimated standard error of 0.22 mm.\n",
        "\n",
        "To create each 98% confidence intervals for the mean response for each combination of predictors, we specify `level = 0.98` and `interval = \"confidence\"` in the `predict` function in the code below."
      ],
      "id": "66b9cb05-bc1b-4fae-ab31-49e3b69a9d11"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict(lmodp, newdata = newpenguins, level = 0.98, interval = \"confidence\")"
      ],
      "id": "7c367b77-fb73-455e-be3d-35d8c5f79b1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 98% confidence interval for the mean flipper length of an Adelie penguin with a body mass of 3700.662 g is \\[38.33, 39.25\\]. We are 98% confident that the mean flipper length of Adelie penguins with a body mass of 3700.662 is between 38.33 g and 39.25 g. Note: we are constructing a confidence interval for the mean flipper length of ALL Adelie penguins with this body mass, i.e., for:\n",
        "\n",
        "$$\n",
        "E(\\mathtt{flipper\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g} = 3700.662, \\mathtt{species} = \\mathtt{Adelie}),\n",
        "$$\n",
        "\n",
        "which is an unknown characteristic of the population of all Adelie penguins. Similarly, the 98% confidence interval for the mean flipper length of Chinstrap penguins with a body mass of 3733.09 g is \\[48.15, 49.52\\]. Lastly, the mean flipper length of Gentoo penguins with a body mass of 5076.02 g is \\[46.99, 48.02\\].\n",
        "\n",
        "We provide details about manually computing confidence intervals for the mean response in the Going Deeper section.\n",
        "\n",
        "We are once again faced with a multiple comparisons problem because we are making 3 inferences. To control the family-wise confidence level of our intervals, we can use the Bonferroni or Working-Hotelling corrections previously discussed. Both corrections are implemented in the `predict_adjust` function in the **api2lm** package, which is intended to work identically to the `predict` function, but produces intervals that adjust for multiple comparisons. The only additional argument is `method`, with choices `\"none\"` (no correction), `\"bonferroni\"` (Bonferroni adjustment), or `\"wh\"` (Working-Hotelling adjustment). We produce both types of adjusted intervals in the code below. The Bonferroni-adjusted confidence intervals are slightly narrower in this example."
      ],
      "id": "5a7f2abf-95f3-43d8-9361-c19015ed38c2"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bonferroni-adjusted confidence intervals for the mean response\n",
        "predict_adjust(lmodp, newdata = newpenguins, level = 0.98,\n",
        "               interval = \"confidence\", method = \"bonferroni\")\n",
        "# working-hotelling-adjusted confidence intervals for the mean response\n",
        "predict_adjust(lmodp, newdata = newpenguins, level = 0.98,\n",
        "               interval = \"confidence\", method = \"wh\")"
      ],
      "id": "79368826-7861-4271-a22e-42000cec4318"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction interval for a new response\n",
        "\n",
        "We once again assume the regression model:\n",
        "\n",
        "$$\n",
        "E(Y|\\mathbb{X})=\\beta_0+\\beta_1 X_1 + \\ldots \\beta_{p-1} X_{p-1}.\n",
        "$$\n",
        "\n",
        "We want to predict a new response for a specific combination of regressor values, $\\mathbf{x}_0$. The new response will be the mean response for that combination of regressors, $E(Y\\mid \\mathbf{x}_0)$, plus the error for that observation, $\\epsilon(\\mathbf{x}_0)$, i.e.:\n",
        "\n",
        "$$\n",
        "Y(\\mathbf{x}_0)=E(Y\\mid \\mathbf{x}_0) + \\epsilon(\\mathbf{x}_0).\n",
        "$$\n",
        "\n",
        "Thus, our predicted new response, $\\hat{Y}(\\mathbf{x})$ is the estimated mean plus the estimated error, i.e.:\n",
        "\n",
        "$$\n",
        "\\hat{Y}(\\mathbf{x}_0)=\\hat{E}(Y\\mid \\mathbf{x}_0) + \\hat{\\epsilon}(\\mathbf{x}_0).\n",
        "$$\n",
        "\n",
        "We have already used $\\hat{E}(Y\\mid \\mathbf{x}_0)=\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}$ as the estimator for $E(Y\\mid \\mathbf{x}_0)$ since it is the best linear unbiased estimator according to the Gauss-Markov theorem. What should we use for $\\hat{\\epsilon}(\\mathbf{x}_0)$? In short, because all of the errors, $\\epsilon_1, \\epsilon_2,\\ldots, \\epsilon_n, \\epsilon(\\mathbf{x}_0)$ are uncorrelated, our best guess is the mean value of the errors, which is zero. This is not true when the errors are correlated, but we do not discuss that in detail. Thus, the best predictor of a new response is:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{Y}(\\mathbf{x}_0) &=\\hat{E}(Y\\mid \\mathbf{x}_0) + \\hat{\\epsilon}(\\mathbf{x}_0)\\\\\n",
        "&= \\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}} + 0 \\\\\n",
        "&= \\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We want to create a prediction interval for $Y(\\mathbf{x}_0)$. A prediction interval is basically the same thing as a confidence interval, but the interval estimator is for a random variable instead of a parameter. If we divide the estimation error of the new response, i.e., $Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)$, by its estimated standard deviation, $\\widehat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))$, then we obtain a $t$-distributed pivotal quantity:\n",
        "\n",
        "$$\n",
        "\\frac{Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)}{\\widehat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))} \\sim t_{n-p}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\widehat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))=\\hat{\\sigma}\\sqrt{1 + \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0}\n",
        "$$\n",
        "\n",
        "The standard deviation of the prediction error is sometimes known as the **standard error of prediction**, but we do not use that terminology. A detailed derivation of $\\widehat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))$ is provided in the Going Deeper section.\n",
        "\n",
        "There is a conceptual and mathematical relationship between the standard deviation of the estimation error for the mean response and the prediction error for a new observation. If we compare the expression for $\\hat{\\mathrm{se}}(\\hat{E}(Y\\mid \\mathbf{x}_0)) - E(Y\\mid \\mathbf{x}_0))$ in the first equation and $\\widehat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))$ in the second equation, we see that $\\widehat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))$ has an extra “1 +” under the square root. Because of this, *prediction intervals for a new response are always wider than a confidence interval for the mean response* when considering the same regressor values, $\\mathbf{x}_0$, and confidence level. Conceptually, prediction intervals are wider because there are two sources of uncertainty in our prediction: estimating the mean response and predicting the error of the new response. Estimating the mean response does not require us to predict the error of a new response, so the uncertainty of our estimate is less. We can see that this is formally true through the derivations provided in the Going Deeper section.\n",
        "\n",
        "A prediction interval for $Y(\\mathbf{x}_0)$ with confidence level $1-\\alpha$ is given by the expression:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}} \\pm t^{\\alpha/2}_{n-p} \\hat{\\mathrm{sd}}(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0))\n",
        "$$\n",
        "\n",
        "The `predict` function can be used to create predictions and prediction intervals for a new response in the same way that it can be used to estimate the mean response and produce associated confidence intervals. If the `interval` argument is `\"none\"`, then predictions for a new response are returned. As we have already seen, $\\hat{E}(Y\\mid \\mathbf{x}_0)$ and $\\hat{Y}(\\mathbf{x}_0)$ are the same number. If the `interval` argument is `\"prediction\"`, then the predicted new response and associated prediction interval will be produced for each row of data supplied to the `newdata` argument.\n",
        "\n",
        "Continuing the example from before, we want to predict the response value for new, unobserved penguins having the observed mean `body_mass_g` for each level of `species`. The fitted model is stored in `lmodp` and the data frame with the predictors for the new responses is stored in `newpenguins`. We print the coefficients of the fitted model and the data frame of new predictors below for clarity."
      ],
      "id": "39a65e3f-cf7f-4730-8c1f-516c57de081a"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmodp)\n",
        "newpenguins"
      ],
      "id": "d985abab-fc87-402f-a4b7-e37529d95cd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code below, we predict the `flipper_length_mm` for new penguins for the predictor values stored in `newpenguins` based on the fitted model in `lmodp`. We also construct the associated 99% prediction intervals."
      ],
      "id": "c3182e7f-bf76-4b40-8665-28c1ae21dfbd"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict new response and compute prediction intervals\n",
        "# for 3 combinations of predictors\n",
        "predict(lmodp, newdata = newpenguins,\n",
        "        interval = \"prediction\", level = 0.99)"
      ],
      "id": "dea65665-e7e4-4015-955b-671778b86497"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An new Adelie penguin with a body mass of 3700.662 g is predicted to have a flipper length of 38.79 mm. We are 99% confident that a new Adelie penguins with a body mass of 3700.662 g will have a flipper length between 32.54 mm and 45.04 mm. Similarly, the 99% prediction interval for the flipper length of a new Chinstrap penguin with a body mass of 3733.09 g is \\[48.56, 55.11\\]. The flipper length of a new Gentoo penguin with a body mass of 5076.02 g is between 41.25 and 53.75 with a confidence level of 0.99.\n",
        "\n",
        "Since we are making 3 predictions, our inferences suffer from the multiple comparisons problem. To control the family-wise confidence level of our intervals, we can use the Bonferroni correction with $k=3$. The Working-Hotelling correction does not apply to new responses. However, a similar adjustment proposed by Scheffé does apply \\[@alsm2005\\]. The prediction interval multiplier $t^{\\alpha/2}_{n-p}$ used for a single prediction interval with confidence level $1-\\alpha$ is replaced by $\\sqrt{k F^{1-\\alpha}_{k,n-p}}$ to control the family-wise confidence level at $1-\\alpha$ for a family of $k$ prediction intervals. Recall that the Working-Hotelling multiplier was $\\sqrt{p F^{1-\\alpha}_{p,n-p}}$. Thus, the Scheffé multiplier scales with the number of predictions being made while the Working-Hotelling multiplier scales with the number of estimated regression coefficients in the fitted model.\n",
        "\n",
        "Both the Bonferroni and Scheffé multiple comparisons corrections are implemented in the `predict_adjust` function in the **api2lm** package. In the prediction interval setting, we can choose the correction `method` argument to be `\"none\"` (no correction), `\"bonferroni\"` (Bonferroni adjustment), or `\"scheffe\"` (Scheffe adjustment). We produce both types of adjusted intervals in the code below. The Bonferroni-adjusted confidence intervals are slightly narrower in this example."
      ],
      "id": "9d695e18-bb01-4fda-af8b-527d58c542dc"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bonferroni-adjusted prediction intervals for new responses\n",
        "predict_adjust(lmodp, newdata = newpenguins, level = 0.99,\n",
        "               interval = \"prediction\",\n",
        "               method = \"bonferroni\")\n",
        "# sheffe-adjusted prediction intervals for new responses\n",
        "predict_adjust(lmodp, newdata = newpenguins, level = 0.99,\n",
        "               interval = \"prediction\",\n",
        "               method = \"scheffe\")"
      ],
      "id": "799054ce-d1d5-42fb-b47a-f29189b8e1a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hypothesis tests for a single regression coefficient\n",
        "\n",
        "If we assume we are fitting the multiple linear regression model in Equation @ref(eq:model-def-inference), then our model has $p$ regression coefficient $\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}$. How would we perform a hypothesis test for exactly one of these regression coefficients?\n",
        "\n",
        "Recall that if we make the assumptions in Equation @ref(eq:error-assumption-inference), then\n",
        "\n",
        "Let’s say we wish to test whether, for this model, $\\beta_j$ differs from some constant number $c$ (typically, $c=0$). Let $\\boldsymbol{\\beta}_{-j}=\\boldsymbol{\\beta}\\setminus{\\beta_j}$, i.e., $\\boldsymbol{\\beta}_{-j}$ is the vector of all regression coefficients except $\\beta_j$. $\\boldsymbol{\\beta}_{-j}$ has $p-1$ elements. We can state the hypotheses we wish to test as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H_0: \\beta_j &= c \\mid \\boldsymbol{\\beta}_{-j}\\in\\mathbb{R}^{p-1} \\\\\n",
        "H_a: \\beta_j &\\neq c \\mid \\boldsymbol{\\beta}_{-j}\\in\\mathbb{R}^{p-1}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The first part of the null hypothesis in Equation @ref(eq:betaj-H0Ha) states that $\\beta_j = c$ while the first part of the alternative hypothesis assumes that $\\beta_j \\neq c$. But what does the second half of the hypotheses mean? The vertical bar means “assuming or conditional on”, similar to the notation you would see in conditional probabilities or distributions. What this means is that we are performing the hypothesis test while assuming that the other coefficients are some real number (possibly even zero). Why state this at all? Isn’t it implicitly assumed? No. We could perform a hypothesis test for $\\beta_j$ for many different models that have differing regressors. By writing out hypotheses in the style of Equation @ref(eq:betaj-H0Ha), we are being very clear that we are performing a test for $\\beta_j$ in the context of the model that has the regressors associated with $\\boldsymbol{\\beta}_{-j}$.\n",
        "\n",
        "How do the hypotheses in Equation @ref(eq:betaj-H0Ha) affect the pivotal quantity in Equation @ref(eq:pivot-t-betaj)? Under the null hypothesis, the statistic:\n",
        "\n",
        "$$\n",
        "T_j = \\frac{\\hat{\\beta}_j-c}{\\hat{\\mathrm{se}}(\\hat{\\beta}_j)} \\sim t_{n-p}\n",
        "$$\n",
        "\n",
        "Thus, the null distribution of $T_j$ is a $t$ distribution with $n-p$ degrees of freedom. We emphasize that this is only true if the assumptions in Equation @ref(eq:error-assumption-inference) are true.\n",
        "\n",
        "The p-value associated with this test is computed via the equation:\n",
        "\n",
        "$$\n",
        "p\\text{-value} = 2P(t_{n-p}\\geq |T_j|).\n",
        "$$\n",
        "\n",
        "What if we wished to test a one-sided hypothesis? The p-value for a lower-tailed test is $p\\text{-value}=P(t_{n-p}\\leq T_j)$ while the p-value for an upper-tailed test is $p\\text{-value}=P(t_{n-p}\\geq T_j)$.\n",
        "\n",
        "The fitted model $$\n",
        "\\begin{aligned}\n",
        "&E(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g}, \\mathtt{flipper\\_length\\_mm}) \\\\\n",
        "&=\\beta_0+\\beta_1 \\mathtt{body\\_mass\\_g} + \\beta_2 \\mathtt{flipper\\_length\\_mm}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "is assigned the name `mlmod`. Suppose we want to test\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H_0: &\\beta_1 = 0 \\mid \\beta_0\\in \\mathbb{R}, \\beta_2 \\in \\mathbb{R} \\\\\n",
        "H_a: &\\beta_1 \\neq 0 \\mid \\beta_0\\in \\mathbb{R}, \\beta_2 \\in \\mathbb{R}.\n",
        "\\end{aligned}\n",
        "$$ R conveniently provides this information in the output of the `summary` function (specifically, the `coefficients` element if we don’t want extra information). We extract this information from the `mlmod` object in the code below."
      ],
      "id": "14a182a2-491a-43ee-b39b-b8485d2a486d"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(mlmod)$coefficients"
      ],
      "id": "c872ac6c-155c-4679-8a01-9ccf5dd8b99a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving from left to right, the first column (no name) indicates the coefficient term under consideration, the second column (`Estimate`) provides the estimated coefficients, the third column (`Std. Error`) provides the estimated standard error ($\\hat{\\mathrm{se}}(\\hat{\\beta}_j)$) for each coefficient, the fourth column (`t value`) provides the test statistic associated with testing $H_0: \\beta_j = 0 \\mid \\boldsymbol{\\beta}_{-j} \\in \\mathbb{R}^{p-1}$ versus a suitable alternative for each coefficient, while the final column (`Pr(>|t|)`) provides the two-tailed p-value associated with this test.\n",
        "\n",
        "Thus, for our test of the `body_mass_g` coefficient, the test statistic is $T_2 = 1.17$ and the associated p-value is 0.024. There is moderate evidence that the coefficient for `body_mass_g` differs from zero, assuming the model also allows for the inclusion of the intercept and `flipper_length_mm` coefficients.\n",
        "\n",
        "## Hypothesis tests for multiple regression coefficients\n",
        "\n",
        "Suppose we have have a standard linear regression model with $p$ regression coefficients such as the one defined in Equation @ref(eq:model-def-inference). We refer to this model as the “Complete Model”.\n",
        "\n",
        "We want to compare the Complete Model to a “Reduced Model” that is a special case of the Complete Model. The most common examples of Reduced Model are:\n",
        "\n",
        "1.  All of the coefficients except the intercept are set to zero.\n",
        "2.  Some of the coefficients are set to zero.\n",
        "\n",
        "Less commonly considered examples of a Reduced Model set certain coefficients equal to each other or place other restrictions on the coefficients. We emphasize that the Reduced Model is a special case of the Complete Model. Alternatively, this relationship is sometimes described by saying the Reduced Model is *nested* in the Complete Model.\n",
        "\n",
        "Let $RSS(RM)$ denote the RSS of the Reduced Model and $RSS(CM)$ denote the RSS of the Complete Model. Similarly, $df(RM)$ and $df(CM)$ denote the degrees of freedom associated with the RSS for the Reduced and Complete Models, respectively. Recall that the degrees of freedom associated with a RSS is simply $n$, the number of observations used to fit the model, minus the number of estimated regression coefficients in the model being considered.\n",
        "\n",
        "We want to perform a hypothesis test involving multiple regression coefficients in our model. Without be specific or developing complex notation, we cannot be precise in stating the hypotheses we wish to test. In the discussion below, we use RM as an abbreviation for the Reduced Model and CM as an abbreviation for the Complete Model. A general statement of the hypotheses we will test are:\n",
        "\n",
        "$H_0: \\text{RM is an adequate model for describing the population}$\n",
        "\n",
        "$H_a: \\text{FM is a more appropriate model for describing the population.}$\n",
        "\n",
        "We wish to statistically assess whether we can simplify our model to $RM$. If $RM$ doesn’t adequately explain the patterns of the data, then we will conclude that $FM$ is a more appropriate model.\n",
        "\n",
        "If we assume that the assumptions in Equation @ref(eq:error-assumption-inference) are true for $RM$ and that $RM$ is the true model, then\n",
        "\n",
        "$$\n",
        "F=\\frac{\\frac{RSS_{RM}-RSS_{FM}}{df_{RM}-df_{FM}}}{\\frac{RSS_{FM}}{df_{FM}}}=\\frac{\\frac{RSS_{RM}-RSS_{FM}}{df_{RM}-df_{FM}}}{\\hat{\\sigma}^2_{FM}}\\sim F_{df_{RM}-df_{FM},df_{FM}}\n",
        "$$\n",
        "\n",
        "where $F_{df_{RM}-df_{FM},df_{FM}}$ is an $F$ random variable with $df_{RM}-df_{FM}$ numerator degrees of freedom and $df_{FM}$ denominator degrees of freedom and $\\hat{\\sigma}^2_{FM}$ is the estimated error variance of $FM$.\n",
        "\n",
        "The p-value for this test is\n",
        "\n",
        "$$\n",
        "p\\text{-value}=P(F\\geq F_{df_{RM}-df_{FM},df_{FM}}).\n",
        "$$ We consider two common uses for this test below.\\`\n",
        "\n",
        "**Test for a regression relationship**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The most common test involving multiple parameters is to test whether *any* of the non-intercept regression coefficients different from zero.\n",
        "\n",
        "The is known as the “test for a regression relationship”.\n",
        "\n",
        "In this situation, the hypotheses to be tested may be stated as\n",
        "\n",
        "$H_0: E(Y \\mid \\mathbb{X}) = \\beta_0$ $H_a: E(Y\\mid \\mathbb{X})) = \\beta_0 + X_1\\beta_1 + \\cdots + X_{p-1}\\beta_{p-1}.$\n",
        "\n",
        "Alternatively, we can state these hypotheses as\n",
        "\n",
        "$H_0: \\beta_1 = \\cdots = \\beta_{p-1} = 0 \\mid \\beta_0 \\in \\mathbb{R}$ $H_a: \\beta_0 \\in \\mathbb{R}, \\beta_1 \\in \\mathbb{R}, \\ldots, \\beta_{p-1}\\in \\mathbb{R}$.\n",
        "\n",
        "Notice that the Reduced Model in $H_0$ is clearly a special case of the Complete Model with most of the regression coefficients set equal to zero. Notice that we specifically conditioned are hypotheses on the intercept coefficient, $\\beta_0$ being included in both models we are comparing. We want to choose between the models specified in $H_0$ and $H_a$.\n",
        "\n",
        "The $F$ statistic in Equation @ref(eq:f-stat-lh) used for this test simplifies dramatically when performing a test for a regression relationship.Using a bit of calculus or by carefully using the OLS estimator of the regression coefficients, it is possible to show that for the Reduced Model that $\\hat{\\beta}_0=\\bar{Y}$. Thus, for the $RSS_R = \\sum_{i=1}^n (Y_i - \\bar{Y})^2$, which is the definition of the TSS defined in Chapter @ref(linear-model-estimation)! Then the numerator of our F statistic becomes $TSS - RSS_C$, which is mathematically equivalent to the regression sum of squares for the Complete Model. The degrees of freedom of $RSS_R = n - 1$. Similarly, the degrees of freedom of $RSS_C = n - p$. Thus $df_R - df_C = (n - 1) - (n - p) = p-1$. Thus, our F statistic for this test simplifies to\n",
        "\n",
        "$$\n",
        "F = \\frac{SS_{reg}(CM)/(p-1)}{RSS(CM)/(n-p)}= \\frac{SS_{reg}(CM)/(p-1)}{\\hat{\\sigma}^2_{CM}}.\n",
        "$$"
      ],
      "id": "5e87aa2f-1b4d-4086-b1b1-daa86e3f6cab"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<!-- linear-model-estimation -->"
      ],
      "id": "62833ada-fa2d-41c4-b991-dc2438b0d087"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Going Deeper\n",
        "\n",
        "**Manual calculation of the $t$-based confidence interval**\n",
        "\n",
        "Consider the summary of the fitted linear model below, which summarizes a first-order linear model fit to the `penguins` data earlier in this chapter."
      ],
      "id": "87f055e9-6d49-45d5-b536-2c244eca7365"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(mlmod)"
      ],
      "id": "1d3d6a19-6c5a-412f-8d79-b76328aa3821"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to manually produce 95% confidence intervals for the true regression coefficients for this model using Equation @ref(eq:t-ci-betas), then we need to acquire some basic information. We need:\n",
        "\n",
        "-   the estimated regression coefficients\n",
        "-   the degrees of freedom for the fitted model $n-p$\n",
        "-   the $1-\\alpha/2$ quantile of a $t$ random variable with $n-p$ degrees of freedom\n",
        "-   the estimated standard error of the estimated regression coefficients.\n",
        "\n",
        "The estimated regression coefficients can be extracted from our fitted model, `mlmod`, using the `coef` function. We extract and print the estimated coefficients using the code below while simultaneously assigning the vector the name `betahats`."
      ],
      "id": "ac981355-cd98-44d7-8ba5-a740fc05cb7e"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract estimated coefficients from mlmod\n",
        "(betahats <- coef(mlmod))"
      ],
      "id": "daddde6b-951a-437f-b506-4cb60d434833"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The degrees of freedom $n-p$ is referred to as the residual degrees of freedom and can be obtained by using the `df.residual` function on the fitted model. Using the code below, we see that the residual degrees of freedom is 339 (this information was also in the summary of the fitted model)."
      ],
      "id": "e98a6d80-b14b-4c62-92d2-30a25ae68f0b"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.residual(mlmod)"
      ],
      "id": "b9821f62-d30a-42a3-b8bd-017b6aea27a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct a 95% confidence interval for our coefficients, we need to determine the $0.975$ quantile of a $t$ random variable with 339 degrees of freedom. This can be found using the `qt` function, as is done in the code below. We assign this value the name `mult`. Notice that value is a bit above 1.96, which is the value often seen in introductory statistics courses for confidence intervals based on the standard normal distribution ($\\mathsf{N}(0,1)$ distribution)."
      ],
      "id": "7024a4be-c987-4beb-a77c-a6d9eae49264"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "(mult <- qt(0.975, df = 339))"
      ],
      "id": "0b8007a6-1d80-4bc3-9074-d11cef4757af"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimated standard errors of each coefficient are shown in the summary of the fitted model. They are (approximately) 4.58, 0.00057, and 0.032 and can be obtained by extracting the 2nd column of the `coefficients` element produced by the `summary` function."
      ],
      "id": "bf5f6073-e623-4dbe-9b05-4cff4c1a37e9"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "(sehats <- summary(mlmod)$coefficients[,2])"
      ],
      "id": "2b4964d1-6f3a-4ea2-99fd-0d58b8f7f7b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A alternative approach is to use 3 step process below:\n",
        "\n",
        "1.  Use the `vcov` function to obtain the estimated variance matrix of $\\hat{\\boldsymbol{\\beta}}$, i.e., $\\hat{\\mathrm{var}}(\\hat{\\boldsymbol{\\beta}})$.\n",
        "2.  Use the `diag` function to extract the diagonal elements of this matrix, which gives us $\\hat{\\mathrm{var}}(\\hat{\\beta}_0), \\hat{\\mathrm{var}}(\\hat{\\beta}_1), \\ldots, \\hat{\\mathrm{var}}(\\hat{\\beta}_{p-1})$.\n",
        "3.  Use the `sqrt` function to calculate the estimated standard errors from the estimated variances of the estimated coefficients.\n",
        "\n",
        "We use this process in the code below, which returns the same estimated standard errors we previously obtained."
      ],
      "id": "142b4541-0c7f-4f30-8a98-ca1979e2618a"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2nd approach to obtaining estimated standard errors\n",
        "sqrt(diag(vcov(mlmod)))"
      ],
      "id": "712493d4-aa6f-43c3-88d1-7eea617b1004"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the estimated coefficients (`betahats`), the appropriate quantile of the $t$ distribution (`mult`), and the estimated standard errors (`sehats`), we can manually produce the standard t-based confidence intervals using the code below."
      ],
      "id": "ea248fa2-92d8-452a-b754-c170caff3c14"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.frame(lb = betahats - mult * sehats,\n",
        "           ub = betahats - mult * sehats)"
      ],
      "id": "2761dc40-59c2-4223-8ce8-979cf0390a43"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Details about estimation of the mean response**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Consider the estimated mean response for a specific combination of regressor values, denoted by $\\mathbf{x}_0$, so that:\n",
        "\n",
        "$$\n",
        "\\hat{E}(Y\\mid \\mathbf{x}_0)=\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}\n",
        "$$\n",
        "\n",
        "Using some of the matrix-related results from Appendix @ref(prob-review) and the result in Equation @ref(eq:prop-betahat), we can determine that:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{var}\\left(\\hat{E}(Y \\mid \\mathbf{x}_0)\\right) &= \\mathrm{var}(\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) \\\\\n",
        "&= \\mathbf{x}_0^T \\mathrm{var}(\\hat{\\boldsymbol{\\beta}})\\mathbf{x}_0\\\\\n",
        "&= \\sigma^2 \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To get the simplest expression for the confidence interval for $E(Y\\mid\\mathbf{x}_0)$, we have to make a number of connections that are often glossed over. We discuss them explicitly. Since the error variance, $\\sigma^2$, in Equation @ref(eq:var-est-mean-response) isn’t known, we replace it with the typical estimator $\\hat{\\sigma}^2=RSS/(n-p)$ to get:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathrm{var}}\\left(\\hat{E}(Y\\mid\\mathbf{x}_0)\\right)=\\hat{\\sigma}^2 \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0\n",
        "$$\n",
        "\n",
        "The standard error of an estimator is the standard deviation of the estimator’s variance, so we have $\\mathrm{se}\\left(\\hat{E}(Y\\mid\\mathbf{x}_0)\\right)=\\sqrt{\\mathrm{var}\\left(\\hat{E}(Y\\mid \\mathbf{x}_0)\\right)}$. Similarly, we have that the estimated standard error for $\\hat{E}(Y\\mid \\mathbf{x}_0)$ is $\\hat{\\mathrm{se}}\\left(\\hat{E}(Y\\mid\\mathbf{x}_0)\\right)=\\sqrt{\\hat{\\mathrm{var}}\\left(\\hat{E}(Y\\mid \\mathbf{x}_0)\\right)}$. Taking the square root of Equation @ref(eq:est-var-mean), we see that:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathrm{se}}\\left(\\hat{E}(Y\\mid \\mathbf{x}_0)\\right)=\\hat{\\sigma} \\sqrt{\\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0}.\n",
        "$$\n",
        "\n",
        "Additionally, since $E(Y\\mid \\mathbf{x}_0)$ is an (unknown) constant, we also have that:\n",
        "\n",
        "$$\\mathrm{var}\\left(E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)\\right)=\\mathrm{var}\\left(\\hat{E}(Y\\mid \\mathbf{x}_0)\\right).\n",
        "$$\n",
        "\n",
        "If we divide the estimation error of the mean response, i.e., $E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)$, by its estimated standard deviation, then we obtain a pivotal quantity. More specifically, we have:\n",
        "\n",
        "$$\n",
        "\\frac{E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)}{\\sqrt{\\hat{\\mathrm{var}}\\left(E(Y\\mid \\mathbf{x}_0)-\\hat{E}(Y\\mid \\mathbf{x}_0)\\right)}} = \\frac{E(Y\\mid \\mathbf{x}_0)- \\mathbf{x}_0 \\hat{\\boldsymbol{\\beta}}}{ \\hat{\\mathrm{se}}(\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}})}\\sim t_{n-p}.\n",
        "$$\n",
        "\n",
        "**Manual calculation of confidence intervals for the mean response**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We discuss manual computation of the estimated mean response and associated confidence interval for a particular combination of regressor values based on the `penguins` example discussed in Section @ref(parametric-ci-mean-response).\n",
        "\n",
        "Specifically, we estimate the mean response for the fitted parallel lines model given by$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species})\\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 D_C + 3.56 D_G,\n",
        "\\end{aligned}\n",
        "$$ where $D_C$ and $D_G$ denote the indicator variables for the `Chinstrap` and `Gentoo` levels of the `species` variable. This fitted model is stored in `lmodp`. The estimated coefficients are shown in the code below."
      ],
      "id": "2c80cc22-8ae4-4c81-a508-16d8b5ac792c"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmodp)"
      ],
      "id": "6d045c7c-4e2e-47eb-9e79-9192c70c1fe0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to estimate the mean response for the following combination of predictors stored in the `newpenguins` data frame, which is printed in the code below."
      ],
      "id": "f524b651-c2ba-40e2-a6c8-cc60e3fc9689"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean body_mass_g of each species\n",
        "newpenguins"
      ],
      "id": "3b250228-b0d6-473d-8ac5-5a32802d8bdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to use the `newpenguins` data frame to generate the matrix of regressors used to estimate the associated mean responses. We can create the matrix of regressors using the `model.matrix` function. The main arguments of `model.matrix` are:\n",
        "\n",
        "-   `object`: an object of the appropriate class. In our case, it is a formula or fitted model.\n",
        "-   `data`: a data frame with the predictors needed to construct the matrix.\n",
        "\n",
        "We need only the right side of the formula used to fit the model in `lmodp` to create our matrix of regressor values. We can use the `formula` function to extract the formula used to fit `lmodp`. We then use `model.matrix` to create the matrix of regressor by values needed for estimating the mean. The matrix produced by `model.matrix`, `X0`, includes a column for the intercept term and the indicator variables in `lmodp`."
      ],
      "id": "c08e2f10-7e13-4fc6-bdca-7f393f73d2db"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# determine formula used to fit lmodp\n",
        "formula(lmodp)\n",
        "# create matrix of regressor values from newpenguins\n",
        "(X0 <- model.matrix(~ body_mass_g + species, data = newpenguins))"
      ],
      "id": "f18d84f4-c7a4-4050-ad97-00a1fd0a4d5e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can obtain the estimated mean by taking the product of `X0` and the estimated coefficients for `lmodp`. We assign the estimated mean responses the name `est_means`."
      ],
      "id": "d2b165f1-f35f-4b45-aa1c-460357ddc3c9"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "(est_means <- X0 %*% coef(lmodp))"
      ],
      "id": "99ca95f3-82f0-4c14-97f0-68711b853692"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use Equation @ref(eq:sehat-est-mean) to get the estimated standard error of each estimated mean response. First, we use `model.matrix` to extract the original matrix of regressors, $\\mathbf{X}$, from the fitted model `lmodp`. The `sigma` function is used to extract $\\hat{\\sigma}$ from `lmodp`. Each *row* of `X0` contains a particular instance regressor values. In the code below, we extract each row of `X0`, and then use Equation @ref(eq:sehat-est-mean) to compute the estimated standard error associated with each estimated mean response; these are assigned the names `se1`, `se2`, and `se3`. That approach isn’t scalable, so we provide a scalable version of these computations that is assigned the name `sehat`."
      ],
      "id": "147e37a3-d1c2-4158-9107-1f73127c0682"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# original matrix of regressors\n",
        "X <- model.matrix(lmodp)\n",
        "sigmahat <- sigma(lmodp)\n",
        "# compute estimated standard error for each estimated mean response\n",
        "# crossprod(X) = t(X) %*% X\n",
        "(sehat1 <- sigmahat * sqrt(t(X0[1,]) %*% solve(crossprod(X), X0[1,])))\n",
        "(sehat2 <- sigmahat * sqrt(t(X0[2,]) %*% solve(crossprod(X), X0[2,])))\n",
        "(sehat3 <- sigmahat * sqrt(t(X0[3,]) %*% solve(crossprod(X), X0[3,])))\n",
        "# scalable computation of estimated standard errors\n",
        "(sehat <- sigmahat * sqrt(diag(X0 %*% solve(crossprod(X), t(X0)))))"
      ],
      "id": "54497975-3fbd-4205-a93f-b199474f5616"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To finish the computation of our confidence intervals, we determine the correct multiplier, $t_{n-p}^{\\alpha/2}$. For a 98% confidence interval, $\\alpha = 0.02$ and $\\alpha/2 = 0.01$. The degrees of freedom, $n-p$, is 338 (as shown in the code below). So the multiplier can be represented as $t_{338}^{0.01}$, which is the $0.99$ quantile of a $t$ distribution with 338 degrees of freedom. We provide this information to the `qt` function, which provides the quantiles of a $t$ distribution, using the code below to get the correct multiplier."
      ],
      "id": "a73ffac0-95aa-406f-85fd-00102bbf720f"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# degrees of freedom, n-p\n",
        "df.residual(lmodp)\n",
        "# multiplier for confidence interval\n",
        "(mult <- qt(0.99, df = df.residual(lmodp)))"
      ],
      "id": "61777f21-b8f3-4e0d-aad6-63bc76a95cbc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus, our 98% confidence intervals for each mean response can be computed using the code below, which matches the results we obtained in Section @ref(parametric-ci-mean-response)."
      ],
      "id": "8b37027c-e406-4edc-a3de-89e2ef384672"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.frame(lb = est_means - mult * sehat,\n",
        "           ub = est_means + mult * sehat)"
      ],
      "id": "648909a5-56f8-4fdd-a135-8e856727192a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Details about prediction interval for a new response\n",
        "\n",
        "We want to predict the value of a new response for a specific combination of regressor values, $\\mathbf{x}_0$. The value of the new response is denoted by $Y(\\mathbf{x}_0)$ and its prediction by $\\hat{Y}(\\mathbf{x}_0)$.\n",
        "\n",
        "In Section @ref(pi-new-response), we briefly discussed that the new response may be written as:\n",
        "\n",
        "$$\n",
        "Y(\\mathbf{x}_0) = E(Y \\mid \\mathbf{x}_0) + \\epsilon(\\mathbf{x}_0),\n",
        "$$\n",
        "\n",
        "and that the predicted new response, under our standard assumptions, is given by:\n",
        "\n",
        "$$\n",
        "\\hat{Y}(\\mathbf{x}_0)=\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}.\n",
        "$$\n",
        "\n",
        "Using these relationships, we can determine that the variance of the prediction error for a new response is given by:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{var}\\left(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)\\right) &= \\mathrm{var}(Y(\\mathbf{x}_0)-\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) \\\\\n",
        "&= \\mathrm{var}(\\mathbf{x}_0^T\\boldsymbol{\\beta} + \\epsilon(\\mathbf{x}_0)-\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Recall from Appendix @ref(prob-review) that the variance of a constant plus a random variable is equal to the variance of the random variable (since a constant doesn’t vary!). Thus, Equation @ref(eq:var-pred-error1) simplifies to:  \n",
        "\n",
        "$$\n",
        "\\mathrm{var}\\left(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)\\right) = \\mathrm{var}(\\epsilon(\\mathbf{x}_0)-\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}})\n",
        "$$\n",
        "\n",
        "Using results from Appendix @ref(prob-review) related to the variance of a sum of random variables, we have that:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\mathrm{var}\\left(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)\\right) \\\\\n",
        "&= \\mathrm{var}(\\epsilon(\\mathbf{x}_0)-\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) \\\\\n",
        "&= \\mathrm{var}\\left(\\epsilon(\\mathbf{x}_0)\\right)+\\mathrm{var}(-\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) + 2\\mathrm{cov}\\left(\\epsilon(\\mathbf{x}_0), -\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}\\right) \\\\\n",
        "&= \\mathrm{var}\\left(\\epsilon(\\mathbf{x}_0)\\right)+(-1)^2\\mathrm{var}(\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) - 2\\mathrm{cov}\\left(\\epsilon(\\mathbf{x}_0), \\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}\\right) \\\\\n",
        "&= \\mathrm{var}\\left(\\epsilon(\\mathbf{x}_0)\\right)+\\mathrm{var}(\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) - 2\\mathrm{cov}\\left(\\epsilon(\\mathbf{x}_0), \\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}\\right).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The covariance term in the final line of Equation @ref(eq:var-pred-error3) is 0 because $\\epsilon(\\mathbf{x}_0)$ and $\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}$ are uncorrelated. Why are they uncorrelated? Recall that $\\epsilon(\\mathbf{x}_0), \\epsilon_1, \\ldots, \\epsilon_n$ are uncorrelated. Also, recall that $\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$. Since $\\mathbf{y}=[Y_1,Y_2,\\ldots,Y_n]$ and $Y_i=\\mathbf{x}_i^T\\boldsymbol{\\beta} + \\epsilon_i$ for $i=1,2,\\ldots,n$, the “randomness” of $\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}$ comes from $\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n$. Since $\\epsilon(\\mathbf{x}_0)$ is uncorrelated with $\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n$, we conclude that $\\epsilon(\\mathbf{x}_0)$ and $\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}$ are uncorrelated, so their covariance is zero. Thus, we have:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{var}\\left(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)\\right)\n",
        "=\\mathrm{var}\\left(\\epsilon(\\mathbf{x}_0)\\right)+\\mathrm{var}(\\mathbf{x}_0^T \\hat{\\boldsymbol{\\beta}}) .\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "From our assumptions in Section @ref(properties-betahat), we have that $\\mathrm{var}(\\epsilon(\\mathbf{x}_0))=\\sigma^2$. We determined in Section @ref(mean-response-calculations) that:\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}})=\\sigma^2 \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0.\n",
        "$$\n",
        "\n",
        "Using these two facts, we can conclude that:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{var}\\left(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)\\right)\n",
        "&=\\sigma^2 + \\sigma^2 \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0\\\\\n",
        "&=\\sigma^2\\left(1 + \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Replacing $\\sigma^2$ by the typical estimator $\\hat{\\sigma}^2=RSS/(n-p)$ to get the estimated variance of the prediction error and taking the square root of the estimated variance to get the estimated standard deviation of the prediction error, we have that:\n",
        "\n",
        "$$\n",
        "\\widehat{\\mathrm{sd}}\\left(Y(\\mathbf{x}_0)-\\hat{Y}(\\mathbf{x}_0)\\right) = \\hat{\\sigma}\\sqrt{1 + \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{x}_0}\n",
        "$$\n",
        "\n",
        "# References\n",
        "\n",
        "Bonferroni, Carlo. 1936. “Teoria Statistica Delle Classi e Calcolo Delle Probabilita.” Pubblicazioni Del R Istituto Superiore Di Scienze Economiche e Commericiali Di Firenze 8: 3–62.\n",
        "\n",
        "Boole, George. 1847. The Mathematical Analysis of Logic. Philosophical Library.\n",
        "\n",
        "Kutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li. 2005. Applied Linear Statistical Models, 5th Edition. McGraw-Hill/Irwin, New York.\n",
        "\n",
        "Mi, Jie, and Allan R. Sampson. 1993. “A Comparison of the Bonferroni and Scheffé Bounds.” Journal of Statistical Planning and Inference 36 (1): 101–5. https://doi.org/https://doi.org/10.1016/0378-3758(93)90105-F.\n",
        "\n",
        "Wasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Vol. 26. Springer.\n",
        "\n",
        "Weisberg, Sanford. 2014. Applied Linear Regression. Fourth. Hoboken NJ: Wiley. http://z.umn.edu/alr4ed.\n",
        "\n",
        "Working, Holbrook, and Harold Hotelling. 1929. “Applications of the Theory of Error to the Interpretation of Trends.” Journal of the American Statistical Association 24 (165A): 73–85. https://doi.org/10.1080/01621459.1929.10506274."
      ],
      "id": "1a421079-0425-4c3c-8998-e74894c87f41"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "pygments_lexer": "r",
      "version": "4.2.2"
    }
  }
}